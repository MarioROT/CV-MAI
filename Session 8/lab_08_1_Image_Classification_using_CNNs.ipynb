{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioROT/CV-MAI/blob/main/Session%208/lab_08_1_Image_Classification_using_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4rCKcndPybL"
      },
      "source": [
        "# Laboratory #4_1 : Image Classification using CNN\n",
        "\n",
        "At the end of this laboratory, you would get familiarized with\n",
        "\n",
        "*   Creating deep networks using Keras\n",
        "*   Steps necessary in training a neural network\n",
        "*   Prediction and performance analysis using neural networks\n",
        "*   Using pre-trained networks\n",
        "*   Feature visualizations\n",
        "\n",
        "**Remember this is a graded exercise.**\n",
        "\n",
        "*   For every plot, make sure you provide appropriate titles, axis labels, legends, wherever applicable.\n",
        "*   Create reusable functions where ever possible, so that the code could be reused at different places.\n",
        "*   Mount your drive to access the images.\n",
        "*   Add sufficient comments and explanations wherever necessary.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdglSzOi4Cp-"
      },
      "source": [
        "# **Colaboratory environment**\n",
        "By default, Colab notebooks run on CPU.\n",
        "You can switch your notebook to run with GPU.\n",
        "\n",
        "In order to obtain access to the GPU, you need to choose the tab Runtime and then select “Change runtime type” as shown in the following figure:\n",
        "\n",
        "![Changing runtime](https://miro.medium.com/max/747/1*euE7nGZ0uJQcgvkpgvkoQg.png)\n",
        "\n",
        "When a pop-up window appears select GPU. Ensure “Hardware accelerator” is set to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wkicuxZdrdq"
      },
      "source": [
        "# **Working with a new dataset: CIFAR-10**\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More information about CIFAR-10 can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "In Keras, the CIFAR-10 dataset is also preloaded in the form of four Numpy arrays. x_train and y_train contain the training set, while x_test and y_test contain the test data. The images are encoded as Numpy arrays and their corresponding labels ranging from 0 to 9.\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "*   Visualize the images in CIFAR-10 dataset. Create a 10 x 10 plot showing 10 random samples from each class.\n",
        "*   Convert the labels to one-hot encoded form.\n",
        "*   Normalize the images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing custom scripts\n",
        "\n",
        "%%shell\n",
        "git clone https://github.com/mariorot/CV-MAI\n",
        "mv CV-MAI/scripts/* /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4myjJlR_MG3",
        "outputId": "f4ae0b9f-f6c9-4297-f2b7-682f3602226c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CV-MAI'...\n",
            "remote: Enumerating objects: 26321, done.\u001b[K\n",
            "remote: Counting objects: 100% (5248/5248), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5204/5204), done.\u001b[K\n",
            "remote: Total 26321 (delta 49), reused 5234 (delta 44), pack-reused 21073\u001b[K\n",
            "Receiving objects: 100% (26321/26321), 1.25 GiB | 23.95 MiB/s, done.\n",
            "Resolving deltas: 100% (148/148), done.\n",
            "Updating files: 100% (26212/26212), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import data, exposure, filters, io, morphology, color, transform\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import custom_plots as cp"
      ],
      "metadata": {
        "id": "lBgmUofOH47j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Mrb20KGMtTFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b15de3-3ae1-4fdc-eece-3d4c895cf866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "# solution\n",
        "#Getting the image data set, and splitting into train and test, images and labels.\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_img=[]\n",
        "\n",
        "unique_classes = np.unique(y_train)\n",
        "np.random.seed(7)\n",
        "\n",
        "# Iterate through each unique class\n",
        "for img_class in unique_classes:\n",
        "  class_indices = np.where(y_train == img_class)[0]\n",
        "  rand_indices = np.random.choice(class_indices, size=10, replace=False)\n",
        "  rand_img.extend(x_train[rand_indices])\n",
        "\n",
        "# rand_img is now a list containing 10 random samples from each class"
      ],
      "metadata": {
        "id": "ZbBEM_xuLRx5",
        "outputId": "e1d6bc9a-ab73-4617-f16f-fcb395cf90fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c776e75514ee>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrand_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0munique_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the 100 images, 10 random sample for each category\n",
        "plot=cp.custom_grids([],10,10,figsize=(10,10),axis='off',use_grid_spec=False)\n",
        "plot.show()\n",
        "\n",
        "for img in rand_img:\n",
        "  plot.add_plot(axis='off').imshow(img)"
      ],
      "metadata": {
        "id": "lWgIc8Qz-Mpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting labels in one-hot encodding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "GV_tQ2ITLjpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the images\n",
        "x_train=x_train.astype(\"float32\") / 255\n",
        "# x_train=x_train.reshape(50000,3072)\n",
        "\n",
        "x_test=x_test.astype(\"float32\") / 255\n",
        "# x_test=x_test.reshape(10000,3072)"
      ],
      "metadata": {
        "id": "oYePEWBVbrnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "Cj62Jgfwc1Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ER5WlMNRydp"
      },
      "source": [
        "## Define the following model (same as the one in tutorial)\n",
        "\n",
        "**For the convolutional front-end, start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer. Use the input as (32,32,3). The filter maps can then be flattened to provide features to the classifier. Use a dense layer with 100 units before the classification layer (which is also a dense layer with softmax activation).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfWCHxh8HGhN"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSN6riPISBMG"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense, Activation, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "fwRu6u0ouqgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtivbQJT39U"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 512 epochs with a batch size of 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn8UzPBZugVp"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#First run with GPU and took almost an hour\n",
        "model1 = model.fit(x_train, y_train, batch_size=32, epochs=512, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zACjfoX5uzx4",
        "outputId": "6ff7fc9d-1c3e-42cb-a554-f9cf68f66972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/512\n",
            "1407/1407 [==============================] - 16s 5ms/step - loss: 1.8167 - accuracy: 0.3484 - val_loss: 1.7196 - val_accuracy: 0.3820\n",
            "Epoch 2/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4620 - accuracy: 0.4762 - val_loss: 1.3900 - val_accuracy: 0.5106\n",
            "Epoch 3/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3105 - accuracy: 0.5330 - val_loss: 1.4075 - val_accuracy: 0.4994\n",
            "Epoch 4/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2162 - accuracy: 0.5712 - val_loss: 1.2242 - val_accuracy: 0.5670\n",
            "Epoch 5/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1447 - accuracy: 0.5974 - val_loss: 1.2099 - val_accuracy: 0.5758\n",
            "Epoch 6/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0868 - accuracy: 0.6183 - val_loss: 1.1472 - val_accuracy: 0.6000\n",
            "Epoch 7/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 1.0349 - accuracy: 0.6387 - val_loss: 1.2714 - val_accuracy: 0.5654\n",
            "Epoch 8/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9863 - accuracy: 0.6578 - val_loss: 1.1221 - val_accuracy: 0.6102\n",
            "Epoch 9/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.9413 - accuracy: 0.6717 - val_loss: 1.2359 - val_accuracy: 0.5708\n",
            "Epoch 10/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.8980 - accuracy: 0.6872 - val_loss: 1.1460 - val_accuracy: 0.5974\n",
            "Epoch 11/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.8606 - accuracy: 0.7005 - val_loss: 1.0425 - val_accuracy: 0.6354\n",
            "Epoch 12/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.8220 - accuracy: 0.7144 - val_loss: 1.3914 - val_accuracy: 0.5394\n",
            "Epoch 13/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.7872 - accuracy: 0.7296 - val_loss: 1.1511 - val_accuracy: 0.6054\n",
            "Epoch 14/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.7504 - accuracy: 0.7422 - val_loss: 1.0392 - val_accuracy: 0.6442\n",
            "Epoch 15/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.7142 - accuracy: 0.7552 - val_loss: 1.1931 - val_accuracy: 0.6108\n",
            "Epoch 16/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.6818 - accuracy: 0.7659 - val_loss: 1.1541 - val_accuracy: 0.6220\n",
            "Epoch 17/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.6489 - accuracy: 0.7781 - val_loss: 1.0362 - val_accuracy: 0.6532\n",
            "Epoch 18/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.6164 - accuracy: 0.7920 - val_loss: 1.1799 - val_accuracy: 0.6150\n",
            "Epoch 19/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.5830 - accuracy: 0.8028 - val_loss: 1.1212 - val_accuracy: 0.6406\n",
            "Epoch 20/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.5502 - accuracy: 0.8135 - val_loss: 1.3302 - val_accuracy: 0.5956\n",
            "Epoch 21/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.5197 - accuracy: 0.8241 - val_loss: 1.3091 - val_accuracy: 0.6038\n",
            "Epoch 22/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.4913 - accuracy: 0.8355 - val_loss: 1.2628 - val_accuracy: 0.6202\n",
            "Epoch 23/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.4585 - accuracy: 0.8494 - val_loss: 1.1699 - val_accuracy: 0.6426\n",
            "Epoch 24/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.4290 - accuracy: 0.8605 - val_loss: 1.1643 - val_accuracy: 0.6522\n",
            "Epoch 25/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.4005 - accuracy: 0.8709 - val_loss: 1.1503 - val_accuracy: 0.6576\n",
            "Epoch 26/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.3748 - accuracy: 0.8800 - val_loss: 1.2293 - val_accuracy: 0.6476\n",
            "Epoch 27/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.3463 - accuracy: 0.8913 - val_loss: 1.2085 - val_accuracy: 0.6512\n",
            "Epoch 28/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.3200 - accuracy: 0.9005 - val_loss: 1.3805 - val_accuracy: 0.6324\n",
            "Epoch 29/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.2967 - accuracy: 0.9102 - val_loss: 1.4068 - val_accuracy: 0.6292\n",
            "Epoch 30/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.2706 - accuracy: 0.9190 - val_loss: 1.2568 - val_accuracy: 0.6654\n",
            "Epoch 31/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.2484 - accuracy: 0.9284 - val_loss: 1.2768 - val_accuracy: 0.6570\n",
            "Epoch 32/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.2261 - accuracy: 0.9362 - val_loss: 1.6283 - val_accuracy: 0.5976\n",
            "Epoch 33/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.2054 - accuracy: 0.9447 - val_loss: 1.6338 - val_accuracy: 0.6074\n",
            "Epoch 34/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.1852 - accuracy: 0.9524 - val_loss: 1.3843 - val_accuracy: 0.6558\n",
            "Epoch 35/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.1665 - accuracy: 0.9603 - val_loss: 1.7939 - val_accuracy: 0.6252\n",
            "Epoch 36/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.1508 - accuracy: 0.9648 - val_loss: 1.4513 - val_accuracy: 0.6574\n",
            "Epoch 37/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.1342 - accuracy: 0.9711 - val_loss: 1.5351 - val_accuracy: 0.6454\n",
            "Epoch 38/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.1208 - accuracy: 0.9752 - val_loss: 1.5041 - val_accuracy: 0.6532\n",
            "Epoch 39/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.1061 - accuracy: 0.9815 - val_loss: 1.5199 - val_accuracy: 0.6604\n",
            "Epoch 40/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0951 - accuracy: 0.9840 - val_loss: 1.5774 - val_accuracy: 0.6604\n",
            "Epoch 41/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0827 - accuracy: 0.9882 - val_loss: 1.6998 - val_accuracy: 0.6398\n",
            "Epoch 42/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0733 - accuracy: 0.9905 - val_loss: 1.6468 - val_accuracy: 0.6550\n",
            "Epoch 43/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0658 - accuracy: 0.9927 - val_loss: 1.6599 - val_accuracy: 0.6530\n",
            "Epoch 44/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0583 - accuracy: 0.9942 - val_loss: 1.6677 - val_accuracy: 0.6584\n",
            "Epoch 45/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0528 - accuracy: 0.9952 - val_loss: 1.6781 - val_accuracy: 0.6604\n",
            "Epoch 46/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0471 - accuracy: 0.9963 - val_loss: 1.7242 - val_accuracy: 0.6586\n",
            "Epoch 47/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0418 - accuracy: 0.9969 - val_loss: 1.7324 - val_accuracy: 0.6602\n",
            "Epoch 48/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0375 - accuracy: 0.9976 - val_loss: 1.7715 - val_accuracy: 0.6578\n",
            "Epoch 49/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0335 - accuracy: 0.9984 - val_loss: 1.7941 - val_accuracy: 0.6600\n",
            "Epoch 50/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0306 - accuracy: 0.9984 - val_loss: 1.8249 - val_accuracy: 0.6592\n",
            "Epoch 51/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0279 - accuracy: 0.9990 - val_loss: 1.8164 - val_accuracy: 0.6600\n",
            "Epoch 52/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0260 - accuracy: 0.9990 - val_loss: 1.8459 - val_accuracy: 0.6566\n",
            "Epoch 53/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0238 - accuracy: 0.9991 - val_loss: 1.8697 - val_accuracy: 0.6590\n",
            "Epoch 54/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0217 - accuracy: 0.9994 - val_loss: 1.8853 - val_accuracy: 0.6602\n",
            "Epoch 55/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0202 - accuracy: 0.9994 - val_loss: 1.8949 - val_accuracy: 0.6602\n",
            "Epoch 56/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0189 - accuracy: 0.9994 - val_loss: 1.9292 - val_accuracy: 0.6578\n",
            "Epoch 57/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0177 - accuracy: 0.9995 - val_loss: 1.9403 - val_accuracy: 0.6626\n",
            "Epoch 58/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0170 - accuracy: 0.9994 - val_loss: 1.9474 - val_accuracy: 0.6576\n",
            "Epoch 59/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0175 - accuracy: 0.9992 - val_loss: 1.9705 - val_accuracy: 0.6570\n",
            "Epoch 60/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0147 - accuracy: 0.9996 - val_loss: 1.9818 - val_accuracy: 0.6610\n",
            "Epoch 61/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0139 - accuracy: 0.9997 - val_loss: 1.9985 - val_accuracy: 0.6562\n",
            "Epoch 62/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0132 - accuracy: 0.9997 - val_loss: 2.0250 - val_accuracy: 0.6588\n",
            "Epoch 63/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0128 - accuracy: 0.9997 - val_loss: 2.0264 - val_accuracy: 0.6598\n",
            "Epoch 64/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0127 - accuracy: 0.9996 - val_loss: 2.0360 - val_accuracy: 0.6604\n",
            "Epoch 65/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0111 - accuracy: 0.9999 - val_loss: 2.0474 - val_accuracy: 0.6604\n",
            "Epoch 66/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0108 - accuracy: 0.9998 - val_loss: 2.0675 - val_accuracy: 0.6596\n",
            "Epoch 67/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0105 - accuracy: 0.9998 - val_loss: 2.0906 - val_accuracy: 0.6574\n",
            "Epoch 68/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0097 - accuracy: 0.9999 - val_loss: 2.0880 - val_accuracy: 0.6588\n",
            "Epoch 69/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0101 - accuracy: 0.9997 - val_loss: 2.1088 - val_accuracy: 0.6554\n",
            "Epoch 70/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0090 - accuracy: 0.9999 - val_loss: 2.1084 - val_accuracy: 0.6590\n",
            "Epoch 71/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0087 - accuracy: 0.9999 - val_loss: 2.1282 - val_accuracy: 0.6588\n",
            "Epoch 72/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0083 - accuracy: 0.9999 - val_loss: 2.1380 - val_accuracy: 0.6572\n",
            "Epoch 73/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 2.1418 - val_accuracy: 0.6572\n",
            "Epoch 74/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0082 - accuracy: 0.9997 - val_loss: 2.1593 - val_accuracy: 0.6568\n",
            "Epoch 75/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0078 - accuracy: 0.9997 - val_loss: 2.1621 - val_accuracy: 0.6574\n",
            "Epoch 76/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 2.1729 - val_accuracy: 0.6580\n",
            "Epoch 77/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0070 - accuracy: 0.9999 - val_loss: 2.1952 - val_accuracy: 0.6584\n",
            "Epoch 78/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 2.1919 - val_accuracy: 0.6566\n",
            "Epoch 79/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0067 - accuracy: 0.9999 - val_loss: 2.2088 - val_accuracy: 0.6566\n",
            "Epoch 80/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.2137 - val_accuracy: 0.6578\n",
            "Epoch 81/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 2.2190 - val_accuracy: 0.6544\n",
            "Epoch 82/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 2.2255 - val_accuracy: 0.6562\n",
            "Epoch 83/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.2351 - val_accuracy: 0.6564\n",
            "Epoch 84/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 2.2461 - val_accuracy: 0.6586\n",
            "Epoch 85/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.2564 - val_accuracy: 0.6584\n",
            "Epoch 86/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0054 - accuracy: 0.9999 - val_loss: 2.2841 - val_accuracy: 0.6556\n",
            "Epoch 87/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0053 - accuracy: 0.9999 - val_loss: 2.2684 - val_accuracy: 0.6562\n",
            "Epoch 88/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.2745 - val_accuracy: 0.6568\n",
            "Epoch 89/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.2888 - val_accuracy: 0.6556\n",
            "Epoch 90/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.2905 - val_accuracy: 0.6594\n",
            "Epoch 91/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 2.2997 - val_accuracy: 0.6572\n",
            "Epoch 92/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.3033 - val_accuracy: 0.6580\n",
            "Epoch 93/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.3127 - val_accuracy: 0.6578\n",
            "Epoch 94/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 2.3174 - val_accuracy: 0.6550\n",
            "Epoch 95/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 2.3233 - val_accuracy: 0.6574\n",
            "Epoch 96/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.3301 - val_accuracy: 0.6562\n",
            "Epoch 97/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 2.3383 - val_accuracy: 0.6578\n",
            "Epoch 98/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.3462 - val_accuracy: 0.6586\n",
            "Epoch 99/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.3502 - val_accuracy: 0.6566\n",
            "Epoch 100/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.3623 - val_accuracy: 0.6576\n",
            "Epoch 101/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.3626 - val_accuracy: 0.6560\n",
            "Epoch 102/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.3722 - val_accuracy: 0.6592\n",
            "Epoch 103/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.3755 - val_accuracy: 0.6560\n",
            "Epoch 104/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.3806 - val_accuracy: 0.6572\n",
            "Epoch 105/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.3904 - val_accuracy: 0.6550\n",
            "Epoch 106/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 2.3901 - val_accuracy: 0.6554\n",
            "Epoch 107/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 2.4005 - val_accuracy: 0.6574\n",
            "Epoch 108/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.4028 - val_accuracy: 0.6562\n",
            "Epoch 109/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.4102 - val_accuracy: 0.6558\n",
            "Epoch 110/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.4194 - val_accuracy: 0.6570\n",
            "Epoch 111/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.4203 - val_accuracy: 0.6578\n",
            "Epoch 112/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.4233 - val_accuracy: 0.6582\n",
            "Epoch 113/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.4305 - val_accuracy: 0.6582\n",
            "Epoch 114/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.4350 - val_accuracy: 0.6584\n",
            "Epoch 115/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.4379 - val_accuracy: 0.6574\n",
            "Epoch 116/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.4416 - val_accuracy: 0.6572\n",
            "Epoch 117/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.4498 - val_accuracy: 0.6584\n",
            "Epoch 118/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.4528 - val_accuracy: 0.6570\n",
            "Epoch 119/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.4610 - val_accuracy: 0.6572\n",
            "Epoch 120/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.4646 - val_accuracy: 0.6570\n",
            "Epoch 121/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.4706 - val_accuracy: 0.6552\n",
            "Epoch 122/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.4701 - val_accuracy: 0.6588\n",
            "Epoch 123/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.4747 - val_accuracy: 0.6566\n",
            "Epoch 124/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.4805 - val_accuracy: 0.6576\n",
            "Epoch 125/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.4908 - val_accuracy: 0.6560\n",
            "Epoch 126/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.4909 - val_accuracy: 0.6580\n",
            "Epoch 127/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.4938 - val_accuracy: 0.6588\n",
            "Epoch 128/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.4975 - val_accuracy: 0.6590\n",
            "Epoch 129/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.5025 - val_accuracy: 0.6580\n",
            "Epoch 130/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.5072 - val_accuracy: 0.6566\n",
            "Epoch 131/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.5123 - val_accuracy: 0.6584\n",
            "Epoch 132/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.5157 - val_accuracy: 0.6572\n",
            "Epoch 133/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.5218 - val_accuracy: 0.6588\n",
            "Epoch 134/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.5236 - val_accuracy: 0.6562\n",
            "Epoch 135/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.5268 - val_accuracy: 0.6566\n",
            "Epoch 136/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.5320 - val_accuracy: 0.6580\n",
            "Epoch 137/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.5329 - val_accuracy: 0.6576\n",
            "Epoch 138/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.5385 - val_accuracy: 0.6578\n",
            "Epoch 139/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.5436 - val_accuracy: 0.6570\n",
            "Epoch 140/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.5443 - val_accuracy: 0.6570\n",
            "Epoch 141/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.5488 - val_accuracy: 0.6572\n",
            "Epoch 142/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.5545 - val_accuracy: 0.6568\n",
            "Epoch 143/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.5623 - val_accuracy: 0.6550\n",
            "Epoch 144/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.5598 - val_accuracy: 0.6584\n",
            "Epoch 145/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.5648 - val_accuracy: 0.6566\n",
            "Epoch 146/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.5663 - val_accuracy: 0.6588\n",
            "Epoch 147/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.5745 - val_accuracy: 0.6580\n",
            "Epoch 148/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.5757 - val_accuracy: 0.6580\n",
            "Epoch 149/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.5789 - val_accuracy: 0.6578\n",
            "Epoch 150/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.5810 - val_accuracy: 0.6584\n",
            "Epoch 151/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.5836 - val_accuracy: 0.6572\n",
            "Epoch 152/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.5903 - val_accuracy: 0.6546\n",
            "Epoch 153/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.5901 - val_accuracy: 0.6586\n",
            "Epoch 154/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.5935 - val_accuracy: 0.6576\n",
            "Epoch 155/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.6006 - val_accuracy: 0.6562\n",
            "Epoch 156/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.6018 - val_accuracy: 0.6570\n",
            "Epoch 157/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.6031 - val_accuracy: 0.6562\n",
            "Epoch 158/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.6073 - val_accuracy: 0.6582\n",
            "Epoch 159/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.6150 - val_accuracy: 0.6576\n",
            "Epoch 160/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.6146 - val_accuracy: 0.6572\n",
            "Epoch 161/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.6158 - val_accuracy: 0.6562\n",
            "Epoch 162/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.6201 - val_accuracy: 0.6572\n",
            "Epoch 163/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.6239 - val_accuracy: 0.6578\n",
            "Epoch 164/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.6258 - val_accuracy: 0.6582\n",
            "Epoch 165/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.6291 - val_accuracy: 0.6576\n",
            "Epoch 166/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.6332 - val_accuracy: 0.6562\n",
            "Epoch 167/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.6380 - val_accuracy: 0.6578\n",
            "Epoch 168/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.6373 - val_accuracy: 0.6596\n",
            "Epoch 169/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.6405 - val_accuracy: 0.6574\n",
            "Epoch 170/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.6421 - val_accuracy: 0.6576\n",
            "Epoch 171/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.6452 - val_accuracy: 0.6580\n",
            "Epoch 172/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.6502 - val_accuracy: 0.6582\n",
            "Epoch 173/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.6513 - val_accuracy: 0.6576\n",
            "Epoch 174/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.6567 - val_accuracy: 0.6578\n",
            "Epoch 175/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.6598 - val_accuracy: 0.6558\n",
            "Epoch 176/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.6598 - val_accuracy: 0.6578\n",
            "Epoch 177/512\n",
            "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.6642 - val_accuracy: 0.6576\n",
            "Epoch 178/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.6648 - val_accuracy: 0.6580\n",
            "Epoch 179/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.6696 - val_accuracy: 0.6568\n",
            "Epoch 180/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.6772 - val_accuracy: 0.6582\n",
            "Epoch 181/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.6743 - val_accuracy: 0.6566\n",
            "Epoch 182/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.6779 - val_accuracy: 0.6588\n",
            "Epoch 183/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.6804 - val_accuracy: 0.6574\n",
            "Epoch 184/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.6825 - val_accuracy: 0.6576\n",
            "Epoch 185/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.6870 - val_accuracy: 0.6564\n",
            "Epoch 186/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.6881 - val_accuracy: 0.6544\n",
            "Epoch 187/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.6906 - val_accuracy: 0.6586\n",
            "Epoch 188/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.6951 - val_accuracy: 0.6576\n",
            "Epoch 189/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.6966 - val_accuracy: 0.6574\n",
            "Epoch 190/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.6978 - val_accuracy: 0.6566\n",
            "Epoch 191/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.7037 - val_accuracy: 0.6584\n",
            "Epoch 192/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.7024 - val_accuracy: 0.6570\n",
            "Epoch 193/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.7048 - val_accuracy: 0.6584\n",
            "Epoch 194/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.7093 - val_accuracy: 0.6560\n",
            "Epoch 195/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.7132 - val_accuracy: 0.6574\n",
            "Epoch 196/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7130 - val_accuracy: 0.6586\n",
            "Epoch 197/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7130 - val_accuracy: 0.6574\n",
            "Epoch 198/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7179 - val_accuracy: 0.6576\n",
            "Epoch 199/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7218 - val_accuracy: 0.6564\n",
            "Epoch 200/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7230 - val_accuracy: 0.6580\n",
            "Epoch 201/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7258 - val_accuracy: 0.6580\n",
            "Epoch 202/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7288 - val_accuracy: 0.6568\n",
            "Epoch 203/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7295 - val_accuracy: 0.6578\n",
            "Epoch 204/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7351 - val_accuracy: 0.6572\n",
            "Epoch 205/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7343 - val_accuracy: 0.6576\n",
            "Epoch 206/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7389 - val_accuracy: 0.6576\n",
            "Epoch 207/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7393 - val_accuracy: 0.6584\n",
            "Epoch 208/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.7407 - val_accuracy: 0.6584\n",
            "Epoch 209/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.7433 - val_accuracy: 0.6578\n",
            "Epoch 210/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.7451 - val_accuracy: 0.6572\n",
            "Epoch 211/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.7488 - val_accuracy: 0.6574\n",
            "Epoch 212/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.7499 - val_accuracy: 0.6572\n",
            "Epoch 213/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.7515 - val_accuracy: 0.6576\n",
            "Epoch 214/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.7532 - val_accuracy: 0.6574\n",
            "Epoch 215/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 9.9206e-04 - accuracy: 1.0000 - val_loss: 2.7580 - val_accuracy: 0.6574\n",
            "Epoch 216/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 9.8632e-04 - accuracy: 1.0000 - val_loss: 2.7587 - val_accuracy: 0.6582\n",
            "Epoch 217/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 9.8049e-04 - accuracy: 1.0000 - val_loss: 2.7610 - val_accuracy: 0.6582\n",
            "Epoch 218/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 9.7250e-04 - accuracy: 1.0000 - val_loss: 2.7652 - val_accuracy: 0.6564\n",
            "Epoch 219/512\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 9.6693e-04 - accuracy: 1.0000 - val_loss: 2.7665 - val_accuracy: 0.6576\n",
            "Epoch 220/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 9.6047e-04 - accuracy: 1.0000 - val_loss: 2.7668 - val_accuracy: 0.6576\n",
            "Epoch 221/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 9.5357e-04 - accuracy: 1.0000 - val_loss: 2.7697 - val_accuracy: 0.6580\n",
            "Epoch 222/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 9.4829e-04 - accuracy: 1.0000 - val_loss: 2.7733 - val_accuracy: 0.6568\n",
            "Epoch 223/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 9.4075e-04 - accuracy: 1.0000 - val_loss: 2.7748 - val_accuracy: 0.6578\n",
            "Epoch 224/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 9.3367e-04 - accuracy: 1.0000 - val_loss: 2.7749 - val_accuracy: 0.6584\n",
            "Epoch 225/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 9.2739e-04 - accuracy: 1.0000 - val_loss: 2.7776 - val_accuracy: 0.6578\n",
            "Epoch 226/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 9.2254e-04 - accuracy: 1.0000 - val_loss: 2.7797 - val_accuracy: 0.6576\n",
            "Epoch 227/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 9.1555e-04 - accuracy: 1.0000 - val_loss: 2.7812 - val_accuracy: 0.6574\n",
            "Epoch 228/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 9.1114e-04 - accuracy: 1.0000 - val_loss: 2.7838 - val_accuracy: 0.6568\n",
            "Epoch 229/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 9.0478e-04 - accuracy: 1.0000 - val_loss: 2.7869 - val_accuracy: 0.6568\n",
            "Epoch 230/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 8.9936e-04 - accuracy: 1.0000 - val_loss: 2.7877 - val_accuracy: 0.6576\n",
            "Epoch 231/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 8.9318e-04 - accuracy: 1.0000 - val_loss: 2.7906 - val_accuracy: 0.6572\n",
            "Epoch 232/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 8.8655e-04 - accuracy: 1.0000 - val_loss: 2.7945 - val_accuracy: 0.6566\n",
            "Epoch 233/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 8.8200e-04 - accuracy: 1.0000 - val_loss: 2.7931 - val_accuracy: 0.6578\n",
            "Epoch 234/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 8.7543e-04 - accuracy: 1.0000 - val_loss: 2.7957 - val_accuracy: 0.6582\n",
            "Epoch 235/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 8.7079e-04 - accuracy: 1.0000 - val_loss: 2.7992 - val_accuracy: 0.6574\n",
            "Epoch 236/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 8.6557e-04 - accuracy: 1.0000 - val_loss: 2.7989 - val_accuracy: 0.6576\n",
            "Epoch 237/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 8.5852e-04 - accuracy: 1.0000 - val_loss: 2.8009 - val_accuracy: 0.6566\n",
            "Epoch 238/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 8.5443e-04 - accuracy: 1.0000 - val_loss: 2.8028 - val_accuracy: 0.6582\n",
            "Epoch 239/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 8.4962e-04 - accuracy: 1.0000 - val_loss: 2.8061 - val_accuracy: 0.6570\n",
            "Epoch 240/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 8.4426e-04 - accuracy: 1.0000 - val_loss: 2.8079 - val_accuracy: 0.6562\n",
            "Epoch 241/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 8.3770e-04 - accuracy: 1.0000 - val_loss: 2.8122 - val_accuracy: 0.6576\n",
            "Epoch 242/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 8.3459e-04 - accuracy: 1.0000 - val_loss: 2.8122 - val_accuracy: 0.6566\n",
            "Epoch 243/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 8.2727e-04 - accuracy: 1.0000 - val_loss: 2.8139 - val_accuracy: 0.6586\n",
            "Epoch 244/512\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 8.2418e-04 - accuracy: 1.0000 - val_loss: 2.8159 - val_accuracy: 0.6586\n",
            "Epoch 245/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 8.1855e-04 - accuracy: 1.0000 - val_loss: 2.8183 - val_accuracy: 0.6578\n",
            "Epoch 246/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 8.1424e-04 - accuracy: 1.0000 - val_loss: 2.8182 - val_accuracy: 0.6584\n",
            "Epoch 247/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 8.0912e-04 - accuracy: 1.0000 - val_loss: 2.8222 - val_accuracy: 0.6582\n",
            "Epoch 248/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 8.0371e-04 - accuracy: 1.0000 - val_loss: 2.8229 - val_accuracy: 0.6584\n",
            "Epoch 249/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 8.0044e-04 - accuracy: 1.0000 - val_loss: 2.8235 - val_accuracy: 0.6578\n",
            "Epoch 250/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.9485e-04 - accuracy: 1.0000 - val_loss: 2.8257 - val_accuracy: 0.6582\n",
            "Epoch 251/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 7.8982e-04 - accuracy: 1.0000 - val_loss: 2.8294 - val_accuracy: 0.6578\n",
            "Epoch 252/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.8547e-04 - accuracy: 1.0000 - val_loss: 2.8290 - val_accuracy: 0.6582\n",
            "Epoch 253/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 7.8024e-04 - accuracy: 1.0000 - val_loss: 2.8319 - val_accuracy: 0.6578\n",
            "Epoch 254/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.7701e-04 - accuracy: 1.0000 - val_loss: 2.8336 - val_accuracy: 0.6572\n",
            "Epoch 255/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 7.7241e-04 - accuracy: 1.0000 - val_loss: 2.8354 - val_accuracy: 0.6568\n",
            "Epoch 256/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.6858e-04 - accuracy: 1.0000 - val_loss: 2.8376 - val_accuracy: 0.6578\n",
            "Epoch 257/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 7.6389e-04 - accuracy: 1.0000 - val_loss: 2.8387 - val_accuracy: 0.6572\n",
            "Epoch 258/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.5940e-04 - accuracy: 1.0000 - val_loss: 2.8413 - val_accuracy: 0.6578\n",
            "Epoch 259/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 7.5472e-04 - accuracy: 1.0000 - val_loss: 2.8404 - val_accuracy: 0.6592\n",
            "Epoch 260/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.5121e-04 - accuracy: 1.0000 - val_loss: 2.8425 - val_accuracy: 0.6576\n",
            "Epoch 261/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 7.4649e-04 - accuracy: 1.0000 - val_loss: 2.8468 - val_accuracy: 0.6570\n",
            "Epoch 262/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.4241e-04 - accuracy: 1.0000 - val_loss: 2.8475 - val_accuracy: 0.6570\n",
            "Epoch 263/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 7.3883e-04 - accuracy: 1.0000 - val_loss: 2.8509 - val_accuracy: 0.6566\n",
            "Epoch 264/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.3470e-04 - accuracy: 1.0000 - val_loss: 2.8502 - val_accuracy: 0.6574\n",
            "Epoch 265/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.3065e-04 - accuracy: 1.0000 - val_loss: 2.8508 - val_accuracy: 0.6584\n",
            "Epoch 266/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.2570e-04 - accuracy: 1.0000 - val_loss: 2.8530 - val_accuracy: 0.6568\n",
            "Epoch 267/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.2309e-04 - accuracy: 1.0000 - val_loss: 2.8552 - val_accuracy: 0.6574\n",
            "Epoch 268/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.1957e-04 - accuracy: 1.0000 - val_loss: 2.8586 - val_accuracy: 0.6560\n",
            "Epoch 269/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 7.1440e-04 - accuracy: 1.0000 - val_loss: 2.8581 - val_accuracy: 0.6574\n",
            "Epoch 270/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.1110e-04 - accuracy: 1.0000 - val_loss: 2.8610 - val_accuracy: 0.6580\n",
            "Epoch 271/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.0762e-04 - accuracy: 1.0000 - val_loss: 2.8627 - val_accuracy: 0.6580\n",
            "Epoch 272/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 7.0318e-04 - accuracy: 1.0000 - val_loss: 2.8639 - val_accuracy: 0.6570\n",
            "Epoch 273/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.9946e-04 - accuracy: 1.0000 - val_loss: 2.8659 - val_accuracy: 0.6576\n",
            "Epoch 274/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.9665e-04 - accuracy: 1.0000 - val_loss: 2.8675 - val_accuracy: 0.6574\n",
            "Epoch 275/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.9292e-04 - accuracy: 1.0000 - val_loss: 2.8699 - val_accuracy: 0.6572\n",
            "Epoch 276/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 6.8816e-04 - accuracy: 1.0000 - val_loss: 2.8710 - val_accuracy: 0.6584\n",
            "Epoch 277/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.8583e-04 - accuracy: 1.0000 - val_loss: 2.8721 - val_accuracy: 0.6580\n",
            "Epoch 278/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 6.8176e-04 - accuracy: 1.0000 - val_loss: 2.8728 - val_accuracy: 0.6574\n",
            "Epoch 279/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.7866e-04 - accuracy: 1.0000 - val_loss: 2.8755 - val_accuracy: 0.6580\n",
            "Epoch 280/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.7531e-04 - accuracy: 1.0000 - val_loss: 2.8761 - val_accuracy: 0.6586\n",
            "Epoch 281/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.7156e-04 - accuracy: 1.0000 - val_loss: 2.8786 - val_accuracy: 0.6578\n",
            "Epoch 282/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 6.6850e-04 - accuracy: 1.0000 - val_loss: 2.8806 - val_accuracy: 0.6576\n",
            "Epoch 283/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.6430e-04 - accuracy: 1.0000 - val_loss: 2.8821 - val_accuracy: 0.6578\n",
            "Epoch 284/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.6161e-04 - accuracy: 1.0000 - val_loss: 2.8839 - val_accuracy: 0.6580\n",
            "Epoch 285/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.5885e-04 - accuracy: 1.0000 - val_loss: 2.8852 - val_accuracy: 0.6578\n",
            "Epoch 286/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 6.5480e-04 - accuracy: 1.0000 - val_loss: 2.8869 - val_accuracy: 0.6578\n",
            "Epoch 287/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.5167e-04 - accuracy: 1.0000 - val_loss: 2.8891 - val_accuracy: 0.6578\n",
            "Epoch 288/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 6.4893e-04 - accuracy: 1.0000 - val_loss: 2.8885 - val_accuracy: 0.6576\n",
            "Epoch 289/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.4527e-04 - accuracy: 1.0000 - val_loss: 2.8915 - val_accuracy: 0.6576\n",
            "Epoch 290/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 6.4200e-04 - accuracy: 1.0000 - val_loss: 2.8941 - val_accuracy: 0.6564\n",
            "Epoch 291/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.3939e-04 - accuracy: 1.0000 - val_loss: 2.8946 - val_accuracy: 0.6566\n",
            "Epoch 292/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 6.3550e-04 - accuracy: 1.0000 - val_loss: 2.8949 - val_accuracy: 0.6582\n",
            "Epoch 293/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.3290e-04 - accuracy: 1.0000 - val_loss: 2.8963 - val_accuracy: 0.6574\n",
            "Epoch 294/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 6.3058e-04 - accuracy: 1.0000 - val_loss: 2.8972 - val_accuracy: 0.6580\n",
            "Epoch 295/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.2712e-04 - accuracy: 1.0000 - val_loss: 2.9014 - val_accuracy: 0.6574\n",
            "Epoch 296/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 6.2382e-04 - accuracy: 1.0000 - val_loss: 2.9027 - val_accuracy: 0.6578\n",
            "Epoch 297/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.2103e-04 - accuracy: 1.0000 - val_loss: 2.9039 - val_accuracy: 0.6572\n",
            "Epoch 298/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 6.1793e-04 - accuracy: 1.0000 - val_loss: 2.9059 - val_accuracy: 0.6574\n",
            "Epoch 299/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.1477e-04 - accuracy: 1.0000 - val_loss: 2.9071 - val_accuracy: 0.6570\n",
            "Epoch 300/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 6.1215e-04 - accuracy: 1.0000 - val_loss: 2.9070 - val_accuracy: 0.6586\n",
            "Epoch 301/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.0965e-04 - accuracy: 1.0000 - val_loss: 2.9078 - val_accuracy: 0.6578\n",
            "Epoch 302/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 6.0666e-04 - accuracy: 1.0000 - val_loss: 2.9103 - val_accuracy: 0.6576\n",
            "Epoch 303/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 6.0381e-04 - accuracy: 1.0000 - val_loss: 2.9127 - val_accuracy: 0.6570\n",
            "Epoch 304/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 6.0098e-04 - accuracy: 1.0000 - val_loss: 2.9132 - val_accuracy: 0.6574\n",
            "Epoch 305/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.9838e-04 - accuracy: 1.0000 - val_loss: 2.9156 - val_accuracy: 0.6572\n",
            "Epoch 306/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 5.9515e-04 - accuracy: 1.0000 - val_loss: 2.9157 - val_accuracy: 0.6580\n",
            "Epoch 307/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.9289e-04 - accuracy: 1.0000 - val_loss: 2.9161 - val_accuracy: 0.6580\n",
            "Epoch 308/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 5.9008e-04 - accuracy: 1.0000 - val_loss: 2.9190 - val_accuracy: 0.6576\n",
            "Epoch 309/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.8705e-04 - accuracy: 1.0000 - val_loss: 2.9204 - val_accuracy: 0.6574\n",
            "Epoch 310/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.8425e-04 - accuracy: 1.0000 - val_loss: 2.9211 - val_accuracy: 0.6582\n",
            "Epoch 311/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.8239e-04 - accuracy: 1.0000 - val_loss: 2.9255 - val_accuracy: 0.6570\n",
            "Epoch 312/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.7959e-04 - accuracy: 1.0000 - val_loss: 2.9243 - val_accuracy: 0.6572\n",
            "Epoch 313/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.7703e-04 - accuracy: 1.0000 - val_loss: 2.9257 - val_accuracy: 0.6576\n",
            "Epoch 314/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.7412e-04 - accuracy: 1.0000 - val_loss: 2.9269 - val_accuracy: 0.6572\n",
            "Epoch 315/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.7198e-04 - accuracy: 1.0000 - val_loss: 2.9280 - val_accuracy: 0.6572\n",
            "Epoch 316/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.6924e-04 - accuracy: 1.0000 - val_loss: 2.9295 - val_accuracy: 0.6576\n",
            "Epoch 317/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.6699e-04 - accuracy: 1.0000 - val_loss: 2.9339 - val_accuracy: 0.6576\n",
            "Epoch 318/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.6459e-04 - accuracy: 1.0000 - val_loss: 2.9306 - val_accuracy: 0.6576\n",
            "Epoch 319/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.6202e-04 - accuracy: 1.0000 - val_loss: 2.9347 - val_accuracy: 0.6580\n",
            "Epoch 320/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.5913e-04 - accuracy: 1.0000 - val_loss: 2.9364 - val_accuracy: 0.6584\n",
            "Epoch 321/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.5694e-04 - accuracy: 1.0000 - val_loss: 2.9374 - val_accuracy: 0.6576\n",
            "Epoch 322/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.5471e-04 - accuracy: 1.0000 - val_loss: 2.9371 - val_accuracy: 0.6578\n",
            "Epoch 323/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.5213e-04 - accuracy: 1.0000 - val_loss: 2.9421 - val_accuracy: 0.6572\n",
            "Epoch 324/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.4974e-04 - accuracy: 1.0000 - val_loss: 2.9415 - val_accuracy: 0.6570\n",
            "Epoch 325/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.4763e-04 - accuracy: 1.0000 - val_loss: 2.9421 - val_accuracy: 0.6578\n",
            "Epoch 326/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.4502e-04 - accuracy: 1.0000 - val_loss: 2.9449 - val_accuracy: 0.6578\n",
            "Epoch 327/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.4252e-04 - accuracy: 1.0000 - val_loss: 2.9433 - val_accuracy: 0.6572\n",
            "Epoch 328/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.4039e-04 - accuracy: 1.0000 - val_loss: 2.9463 - val_accuracy: 0.6570\n",
            "Epoch 329/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 5.3799e-04 - accuracy: 1.0000 - val_loss: 2.9476 - val_accuracy: 0.6566\n",
            "Epoch 330/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.3581e-04 - accuracy: 1.0000 - val_loss: 2.9482 - val_accuracy: 0.6570\n",
            "Epoch 331/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 5.3330e-04 - accuracy: 1.0000 - val_loss: 2.9499 - val_accuracy: 0.6582\n",
            "Epoch 332/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.3140e-04 - accuracy: 1.0000 - val_loss: 2.9512 - val_accuracy: 0.6572\n",
            "Epoch 333/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 5.2895e-04 - accuracy: 1.0000 - val_loss: 2.9516 - val_accuracy: 0.6570\n",
            "Epoch 334/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.2690e-04 - accuracy: 1.0000 - val_loss: 2.9550 - val_accuracy: 0.6586\n",
            "Epoch 335/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 5.2491e-04 - accuracy: 1.0000 - val_loss: 2.9560 - val_accuracy: 0.6572\n",
            "Epoch 336/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.2229e-04 - accuracy: 1.0000 - val_loss: 2.9562 - val_accuracy: 0.6580\n",
            "Epoch 337/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 5.2068e-04 - accuracy: 1.0000 - val_loss: 2.9583 - val_accuracy: 0.6572\n",
            "Epoch 338/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.1818e-04 - accuracy: 1.0000 - val_loss: 2.9598 - val_accuracy: 0.6578\n",
            "Epoch 339/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.1571e-04 - accuracy: 1.0000 - val_loss: 2.9608 - val_accuracy: 0.6576\n",
            "Epoch 340/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.1383e-04 - accuracy: 1.0000 - val_loss: 2.9616 - val_accuracy: 0.6568\n",
            "Epoch 341/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.1218e-04 - accuracy: 1.0000 - val_loss: 2.9625 - val_accuracy: 0.6578\n",
            "Epoch 342/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.1048e-04 - accuracy: 1.0000 - val_loss: 2.9652 - val_accuracy: 0.6566\n",
            "Epoch 343/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.0810e-04 - accuracy: 1.0000 - val_loss: 2.9664 - val_accuracy: 0.6570\n",
            "Epoch 344/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.0604e-04 - accuracy: 1.0000 - val_loss: 2.9671 - val_accuracy: 0.6576\n",
            "Epoch 345/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 5.0412e-04 - accuracy: 1.0000 - val_loss: 2.9688 - val_accuracy: 0.6576\n",
            "Epoch 346/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 5.0195e-04 - accuracy: 1.0000 - val_loss: 2.9695 - val_accuracy: 0.6570\n",
            "Epoch 347/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 5.0007e-04 - accuracy: 1.0000 - val_loss: 2.9699 - val_accuracy: 0.6592\n",
            "Epoch 348/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.9842e-04 - accuracy: 1.0000 - val_loss: 2.9718 - val_accuracy: 0.6584\n",
            "Epoch 349/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.9600e-04 - accuracy: 1.0000 - val_loss: 2.9715 - val_accuracy: 0.6582\n",
            "Epoch 350/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.9420e-04 - accuracy: 1.0000 - val_loss: 2.9749 - val_accuracy: 0.6576\n",
            "Epoch 351/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.9228e-04 - accuracy: 1.0000 - val_loss: 2.9778 - val_accuracy: 0.6570\n",
            "Epoch 352/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.9026e-04 - accuracy: 1.0000 - val_loss: 2.9763 - val_accuracy: 0.6572\n",
            "Epoch 353/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 4.8845e-04 - accuracy: 1.0000 - val_loss: 2.9770 - val_accuracy: 0.6580\n",
            "Epoch 354/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.8668e-04 - accuracy: 1.0000 - val_loss: 2.9783 - val_accuracy: 0.6582\n",
            "Epoch 355/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 4.8487e-04 - accuracy: 1.0000 - val_loss: 2.9803 - val_accuracy: 0.6574\n",
            "Epoch 356/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.8255e-04 - accuracy: 1.0000 - val_loss: 2.9805 - val_accuracy: 0.6580\n",
            "Epoch 357/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.8097e-04 - accuracy: 1.0000 - val_loss: 2.9833 - val_accuracy: 0.6572\n",
            "Epoch 358/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.7911e-04 - accuracy: 1.0000 - val_loss: 2.9838 - val_accuracy: 0.6578\n",
            "Epoch 359/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.7712e-04 - accuracy: 1.0000 - val_loss: 2.9843 - val_accuracy: 0.6584\n",
            "Epoch 360/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.7580e-04 - accuracy: 1.0000 - val_loss: 2.9854 - val_accuracy: 0.6572\n",
            "Epoch 361/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.7348e-04 - accuracy: 1.0000 - val_loss: 2.9879 - val_accuracy: 0.6576\n",
            "Epoch 362/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.7178e-04 - accuracy: 1.0000 - val_loss: 2.9896 - val_accuracy: 0.6574\n",
            "Epoch 363/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.6978e-04 - accuracy: 1.0000 - val_loss: 2.9896 - val_accuracy: 0.6582\n",
            "Epoch 364/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.6807e-04 - accuracy: 1.0000 - val_loss: 2.9918 - val_accuracy: 0.6582\n",
            "Epoch 365/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.6669e-04 - accuracy: 1.0000 - val_loss: 2.9908 - val_accuracy: 0.6568\n",
            "Epoch 366/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.6513e-04 - accuracy: 1.0000 - val_loss: 2.9931 - val_accuracy: 0.6574\n",
            "Epoch 367/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.6285e-04 - accuracy: 1.0000 - val_loss: 2.9941 - val_accuracy: 0.6586\n",
            "Epoch 368/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.6095e-04 - accuracy: 1.0000 - val_loss: 2.9961 - val_accuracy: 0.6570\n",
            "Epoch 369/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.6002e-04 - accuracy: 1.0000 - val_loss: 2.9978 - val_accuracy: 0.6574\n",
            "Epoch 370/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.5821e-04 - accuracy: 1.0000 - val_loss: 2.9988 - val_accuracy: 0.6580\n",
            "Epoch 371/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.5639e-04 - accuracy: 1.0000 - val_loss: 3.0002 - val_accuracy: 0.6568\n",
            "Epoch 372/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.5447e-04 - accuracy: 1.0000 - val_loss: 3.0007 - val_accuracy: 0.6568\n",
            "Epoch 373/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.5270e-04 - accuracy: 1.0000 - val_loss: 3.0022 - val_accuracy: 0.6580\n",
            "Epoch 374/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 4.5137e-04 - accuracy: 1.0000 - val_loss: 3.0023 - val_accuracy: 0.6578\n",
            "Epoch 375/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.4959e-04 - accuracy: 1.0000 - val_loss: 3.0033 - val_accuracy: 0.6572\n",
            "Epoch 376/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.4812e-04 - accuracy: 1.0000 - val_loss: 3.0040 - val_accuracy: 0.6566\n",
            "Epoch 377/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.4661e-04 - accuracy: 1.0000 - val_loss: 3.0054 - val_accuracy: 0.6572\n",
            "Epoch 378/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 4.4474e-04 - accuracy: 1.0000 - val_loss: 3.0072 - val_accuracy: 0.6564\n",
            "Epoch 379/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.4310e-04 - accuracy: 1.0000 - val_loss: 3.0070 - val_accuracy: 0.6574\n",
            "Epoch 380/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 4.4168e-04 - accuracy: 1.0000 - val_loss: 3.0091 - val_accuracy: 0.6570\n",
            "Epoch 381/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.4020e-04 - accuracy: 1.0000 - val_loss: 3.0101 - val_accuracy: 0.6570\n",
            "Epoch 382/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 4.3860e-04 - accuracy: 1.0000 - val_loss: 3.0121 - val_accuracy: 0.6580\n",
            "Epoch 383/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.3711e-04 - accuracy: 1.0000 - val_loss: 3.0131 - val_accuracy: 0.6578\n",
            "Epoch 384/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 4.3543e-04 - accuracy: 1.0000 - val_loss: 3.0132 - val_accuracy: 0.6578\n",
            "Epoch 385/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.3384e-04 - accuracy: 1.0000 - val_loss: 3.0146 - val_accuracy: 0.6588\n",
            "Epoch 386/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 4.3135e-04 - accuracy: 1.0000 - val_loss: 3.0163 - val_accuracy: 0.6572\n",
            "Epoch 387/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.3089e-04 - accuracy: 1.0000 - val_loss: 3.0168 - val_accuracy: 0.6578\n",
            "Epoch 388/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 4.2956e-04 - accuracy: 1.0000 - val_loss: 3.0183 - val_accuracy: 0.6586\n",
            "Epoch 389/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.2812e-04 - accuracy: 1.0000 - val_loss: 3.0193 - val_accuracy: 0.6586\n",
            "Epoch 390/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 4.2612e-04 - accuracy: 1.0000 - val_loss: 3.0205 - val_accuracy: 0.6580\n",
            "Epoch 391/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.2475e-04 - accuracy: 1.0000 - val_loss: 3.0207 - val_accuracy: 0.6578\n",
            "Epoch 392/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 4.2336e-04 - accuracy: 1.0000 - val_loss: 3.0228 - val_accuracy: 0.6572\n",
            "Epoch 393/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.2190e-04 - accuracy: 1.0000 - val_loss: 3.0236 - val_accuracy: 0.6572\n",
            "Epoch 394/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 4.2041e-04 - accuracy: 1.0000 - val_loss: 3.0248 - val_accuracy: 0.6584\n",
            "Epoch 395/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.1910e-04 - accuracy: 1.0000 - val_loss: 3.0251 - val_accuracy: 0.6582\n",
            "Epoch 396/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 4.1775e-04 - accuracy: 1.0000 - val_loss: 3.0264 - val_accuracy: 0.6582\n",
            "Epoch 397/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.1623e-04 - accuracy: 1.0000 - val_loss: 3.0282 - val_accuracy: 0.6584\n",
            "Epoch 398/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 4.1475e-04 - accuracy: 1.0000 - val_loss: 3.0276 - val_accuracy: 0.6578\n",
            "Epoch 399/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.1306e-04 - accuracy: 1.0000 - val_loss: 3.0295 - val_accuracy: 0.6580\n",
            "Epoch 400/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 4.1221e-04 - accuracy: 1.0000 - val_loss: 3.0308 - val_accuracy: 0.6574\n",
            "Epoch 401/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.1055e-04 - accuracy: 1.0000 - val_loss: 3.0317 - val_accuracy: 0.6578\n",
            "Epoch 402/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 4.0935e-04 - accuracy: 1.0000 - val_loss: 3.0321 - val_accuracy: 0.6578\n",
            "Epoch 403/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.0763e-04 - accuracy: 1.0000 - val_loss: 3.0335 - val_accuracy: 0.6584\n",
            "Epoch 404/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.0654e-04 - accuracy: 1.0000 - val_loss: 3.0355 - val_accuracy: 0.6578\n",
            "Epoch 405/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.0542e-04 - accuracy: 1.0000 - val_loss: 3.0359 - val_accuracy: 0.6572\n",
            "Epoch 406/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.0401e-04 - accuracy: 1.0000 - val_loss: 3.0359 - val_accuracy: 0.6580\n",
            "Epoch 407/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.0242e-04 - accuracy: 1.0000 - val_loss: 3.0376 - val_accuracy: 0.6580\n",
            "Epoch 408/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.0123e-04 - accuracy: 1.0000 - val_loss: 3.0392 - val_accuracy: 0.6578\n",
            "Epoch 409/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 4.0005e-04 - accuracy: 1.0000 - val_loss: 3.0391 - val_accuracy: 0.6584\n",
            "Epoch 410/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.9871e-04 - accuracy: 1.0000 - val_loss: 3.0402 - val_accuracy: 0.6574\n",
            "Epoch 411/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.9680e-04 - accuracy: 1.0000 - val_loss: 3.0415 - val_accuracy: 0.6574\n",
            "Epoch 412/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.9605e-04 - accuracy: 1.0000 - val_loss: 3.0433 - val_accuracy: 0.6584\n",
            "Epoch 413/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.9462e-04 - accuracy: 1.0000 - val_loss: 3.0447 - val_accuracy: 0.6584\n",
            "Epoch 414/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.9328e-04 - accuracy: 1.0000 - val_loss: 3.0445 - val_accuracy: 0.6580\n",
            "Epoch 415/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.9239e-04 - accuracy: 1.0000 - val_loss: 3.0462 - val_accuracy: 0.6566\n",
            "Epoch 416/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.9085e-04 - accuracy: 1.0000 - val_loss: 3.0466 - val_accuracy: 0.6584\n",
            "Epoch 417/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.8964e-04 - accuracy: 1.0000 - val_loss: 3.0497 - val_accuracy: 0.6582\n",
            "Epoch 418/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.8818e-04 - accuracy: 1.0000 - val_loss: 3.0496 - val_accuracy: 0.6566\n",
            "Epoch 419/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.8698e-04 - accuracy: 1.0000 - val_loss: 3.0522 - val_accuracy: 0.6572\n",
            "Epoch 420/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.8582e-04 - accuracy: 1.0000 - val_loss: 3.0518 - val_accuracy: 0.6582\n",
            "Epoch 421/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.8470e-04 - accuracy: 1.0000 - val_loss: 3.0530 - val_accuracy: 0.6574\n",
            "Epoch 422/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.8355e-04 - accuracy: 1.0000 - val_loss: 3.0538 - val_accuracy: 0.6576\n",
            "Epoch 423/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.8233e-04 - accuracy: 1.0000 - val_loss: 3.0543 - val_accuracy: 0.6576\n",
            "Epoch 424/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.8136e-04 - accuracy: 1.0000 - val_loss: 3.0552 - val_accuracy: 0.6574\n",
            "Epoch 425/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.7996e-04 - accuracy: 1.0000 - val_loss: 3.0563 - val_accuracy: 0.6584\n",
            "Epoch 426/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.7894e-04 - accuracy: 1.0000 - val_loss: 3.0560 - val_accuracy: 0.6578\n",
            "Epoch 427/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.7728e-04 - accuracy: 1.0000 - val_loss: 3.0591 - val_accuracy: 0.6586\n",
            "Epoch 428/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.7638e-04 - accuracy: 1.0000 - val_loss: 3.0594 - val_accuracy: 0.6580\n",
            "Epoch 429/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 3.7487e-04 - accuracy: 1.0000 - val_loss: 3.0601 - val_accuracy: 0.6582\n",
            "Epoch 430/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.7404e-04 - accuracy: 1.0000 - val_loss: 3.0621 - val_accuracy: 0.6562\n",
            "Epoch 431/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 3.7290e-04 - accuracy: 1.0000 - val_loss: 3.0612 - val_accuracy: 0.6580\n",
            "Epoch 432/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.7155e-04 - accuracy: 1.0000 - val_loss: 3.0636 - val_accuracy: 0.6568\n",
            "Epoch 433/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.7052e-04 - accuracy: 1.0000 - val_loss: 3.0635 - val_accuracy: 0.6576\n",
            "Epoch 434/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.6934e-04 - accuracy: 1.0000 - val_loss: 3.0647 - val_accuracy: 0.6584\n",
            "Epoch 435/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.6828e-04 - accuracy: 1.0000 - val_loss: 3.0663 - val_accuracy: 0.6576\n",
            "Epoch 436/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.6720e-04 - accuracy: 1.0000 - val_loss: 3.0665 - val_accuracy: 0.6574\n",
            "Epoch 437/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.6600e-04 - accuracy: 1.0000 - val_loss: 3.0684 - val_accuracy: 0.6576\n",
            "Epoch 438/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.6492e-04 - accuracy: 1.0000 - val_loss: 3.0693 - val_accuracy: 0.6572\n",
            "Epoch 439/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 3.6367e-04 - accuracy: 1.0000 - val_loss: 3.0699 - val_accuracy: 0.6580\n",
            "Epoch 440/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.6280e-04 - accuracy: 1.0000 - val_loss: 3.0722 - val_accuracy: 0.6572\n",
            "Epoch 441/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 3.6155e-04 - accuracy: 1.0000 - val_loss: 3.0714 - val_accuracy: 0.6572\n",
            "Epoch 442/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.6043e-04 - accuracy: 1.0000 - val_loss: 3.0715 - val_accuracy: 0.6574\n",
            "Epoch 443/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.5932e-04 - accuracy: 1.0000 - val_loss: 3.0731 - val_accuracy: 0.6582\n",
            "Epoch 444/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.5810e-04 - accuracy: 1.0000 - val_loss: 3.0743 - val_accuracy: 0.6578\n",
            "Epoch 445/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.5719e-04 - accuracy: 1.0000 - val_loss: 3.0760 - val_accuracy: 0.6578\n",
            "Epoch 446/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.5643e-04 - accuracy: 1.0000 - val_loss: 3.0766 - val_accuracy: 0.6580\n",
            "Epoch 447/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.5500e-04 - accuracy: 1.0000 - val_loss: 3.0780 - val_accuracy: 0.6574\n",
            "Epoch 448/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.5401e-04 - accuracy: 1.0000 - val_loss: 3.0786 - val_accuracy: 0.6576\n",
            "Epoch 449/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 3.5315e-04 - accuracy: 1.0000 - val_loss: 3.0790 - val_accuracy: 0.6572\n",
            "Epoch 450/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.5212e-04 - accuracy: 1.0000 - val_loss: 3.0802 - val_accuracy: 0.6572\n",
            "Epoch 451/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 3.5109e-04 - accuracy: 1.0000 - val_loss: 3.0811 - val_accuracy: 0.6586\n",
            "Epoch 452/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.5005e-04 - accuracy: 1.0000 - val_loss: 3.0810 - val_accuracy: 0.6572\n",
            "Epoch 453/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.4879e-04 - accuracy: 1.0000 - val_loss: 3.0826 - val_accuracy: 0.6580\n",
            "Epoch 454/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.4773e-04 - accuracy: 1.0000 - val_loss: 3.0855 - val_accuracy: 0.6572\n",
            "Epoch 455/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 3.4689e-04 - accuracy: 1.0000 - val_loss: 3.0843 - val_accuracy: 0.6582\n",
            "Epoch 456/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.4572e-04 - accuracy: 1.0000 - val_loss: 3.0862 - val_accuracy: 0.6582\n",
            "Epoch 457/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.4501e-04 - accuracy: 1.0000 - val_loss: 3.0860 - val_accuracy: 0.6572\n",
            "Epoch 458/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.4374e-04 - accuracy: 1.0000 - val_loss: 3.0868 - val_accuracy: 0.6570\n",
            "Epoch 459/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.4283e-04 - accuracy: 1.0000 - val_loss: 3.0887 - val_accuracy: 0.6574\n",
            "Epoch 460/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.4175e-04 - accuracy: 1.0000 - val_loss: 3.0896 - val_accuracy: 0.6584\n",
            "Epoch 461/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.4099e-04 - accuracy: 1.0000 - val_loss: 3.0898 - val_accuracy: 0.6582\n",
            "Epoch 462/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.3988e-04 - accuracy: 1.0000 - val_loss: 3.0915 - val_accuracy: 0.6578\n",
            "Epoch 463/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.3908e-04 - accuracy: 1.0000 - val_loss: 3.0924 - val_accuracy: 0.6580\n",
            "Epoch 464/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.3793e-04 - accuracy: 1.0000 - val_loss: 3.0927 - val_accuracy: 0.6576\n",
            "Epoch 465/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.3702e-04 - accuracy: 1.0000 - val_loss: 3.0942 - val_accuracy: 0.6572\n",
            "Epoch 466/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.3614e-04 - accuracy: 1.0000 - val_loss: 3.0944 - val_accuracy: 0.6574\n",
            "Epoch 467/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.3514e-04 - accuracy: 1.0000 - val_loss: 3.0955 - val_accuracy: 0.6576\n",
            "Epoch 468/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.3411e-04 - accuracy: 1.0000 - val_loss: 3.0964 - val_accuracy: 0.6576\n",
            "Epoch 469/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.3314e-04 - accuracy: 1.0000 - val_loss: 3.0962 - val_accuracy: 0.6584\n",
            "Epoch 470/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.3251e-04 - accuracy: 1.0000 - val_loss: 3.0977 - val_accuracy: 0.6578\n",
            "Epoch 471/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.3137e-04 - accuracy: 1.0000 - val_loss: 3.0987 - val_accuracy: 0.6574\n",
            "Epoch 472/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.3032e-04 - accuracy: 1.0000 - val_loss: 3.0994 - val_accuracy: 0.6578\n",
            "Epoch 473/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.2946e-04 - accuracy: 1.0000 - val_loss: 3.1013 - val_accuracy: 0.6570\n",
            "Epoch 474/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.2862e-04 - accuracy: 1.0000 - val_loss: 3.1025 - val_accuracy: 0.6576\n",
            "Epoch 475/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.2745e-04 - accuracy: 1.0000 - val_loss: 3.1026 - val_accuracy: 0.6576\n",
            "Epoch 476/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.2675e-04 - accuracy: 1.0000 - val_loss: 3.1035 - val_accuracy: 0.6574\n",
            "Epoch 477/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.2592e-04 - accuracy: 1.0000 - val_loss: 3.1043 - val_accuracy: 0.6568\n",
            "Epoch 478/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.2520e-04 - accuracy: 1.0000 - val_loss: 3.1053 - val_accuracy: 0.6570\n",
            "Epoch 479/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.2419e-04 - accuracy: 1.0000 - val_loss: 3.1053 - val_accuracy: 0.6578\n",
            "Epoch 480/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.2314e-04 - accuracy: 1.0000 - val_loss: 3.1072 - val_accuracy: 0.6572\n",
            "Epoch 481/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.2232e-04 - accuracy: 1.0000 - val_loss: 3.1069 - val_accuracy: 0.6580\n",
            "Epoch 482/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.2133e-04 - accuracy: 1.0000 - val_loss: 3.1083 - val_accuracy: 0.6570\n",
            "Epoch 483/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.2070e-04 - accuracy: 1.0000 - val_loss: 3.1097 - val_accuracy: 0.6574\n",
            "Epoch 484/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.1940e-04 - accuracy: 1.0000 - val_loss: 3.1102 - val_accuracy: 0.6576\n",
            "Epoch 485/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.1888e-04 - accuracy: 1.0000 - val_loss: 3.1105 - val_accuracy: 0.6582\n",
            "Epoch 486/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.1770e-04 - accuracy: 1.0000 - val_loss: 3.1109 - val_accuracy: 0.6578\n",
            "Epoch 487/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.1712e-04 - accuracy: 1.0000 - val_loss: 3.1119 - val_accuracy: 0.6584\n",
            "Epoch 488/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.1592e-04 - accuracy: 1.0000 - val_loss: 3.1141 - val_accuracy: 0.6574\n",
            "Epoch 489/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.1540e-04 - accuracy: 1.0000 - val_loss: 3.1140 - val_accuracy: 0.6570\n",
            "Epoch 490/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.1463e-04 - accuracy: 1.0000 - val_loss: 3.1153 - val_accuracy: 0.6584\n",
            "Epoch 491/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.1358e-04 - accuracy: 1.0000 - val_loss: 3.1165 - val_accuracy: 0.6570\n",
            "Epoch 492/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.1293e-04 - accuracy: 1.0000 - val_loss: 3.1172 - val_accuracy: 0.6582\n",
            "Epoch 493/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.1207e-04 - accuracy: 1.0000 - val_loss: 3.1181 - val_accuracy: 0.6578\n",
            "Epoch 494/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 3.1111e-04 - accuracy: 1.0000 - val_loss: 3.1180 - val_accuracy: 0.6588\n",
            "Epoch 495/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.1032e-04 - accuracy: 1.0000 - val_loss: 3.1193 - val_accuracy: 0.6582\n",
            "Epoch 496/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 3.0966e-04 - accuracy: 1.0000 - val_loss: 3.1196 - val_accuracy: 0.6582\n",
            "Epoch 497/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.0876e-04 - accuracy: 1.0000 - val_loss: 3.1213 - val_accuracy: 0.6570\n",
            "Epoch 498/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 3.0788e-04 - accuracy: 1.0000 - val_loss: 3.1228 - val_accuracy: 0.6580\n",
            "Epoch 499/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.0719e-04 - accuracy: 1.0000 - val_loss: 3.1238 - val_accuracy: 0.6580\n",
            "Epoch 500/512\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 3.0619e-04 - accuracy: 1.0000 - val_loss: 3.1240 - val_accuracy: 0.6574\n",
            "Epoch 501/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.0548e-04 - accuracy: 1.0000 - val_loss: 3.1246 - val_accuracy: 0.6570\n",
            "Epoch 502/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.0484e-04 - accuracy: 1.0000 - val_loss: 3.1255 - val_accuracy: 0.6566\n",
            "Epoch 503/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.0381e-04 - accuracy: 1.0000 - val_loss: 3.1267 - val_accuracy: 0.6576\n",
            "Epoch 504/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.0312e-04 - accuracy: 1.0000 - val_loss: 3.1278 - val_accuracy: 0.6576\n",
            "Epoch 505/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.0240e-04 - accuracy: 1.0000 - val_loss: 3.1276 - val_accuracy: 0.6572\n",
            "Epoch 506/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.0155e-04 - accuracy: 1.0000 - val_loss: 3.1294 - val_accuracy: 0.6572\n",
            "Epoch 507/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 3.0085e-04 - accuracy: 1.0000 - val_loss: 3.1294 - val_accuracy: 0.6570\n",
            "Epoch 508/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 3.0006e-04 - accuracy: 1.0000 - val_loss: 3.1313 - val_accuracy: 0.6568\n",
            "Epoch 509/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 2.9926e-04 - accuracy: 1.0000 - val_loss: 3.1300 - val_accuracy: 0.6584\n",
            "Epoch 510/512\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 2.9833e-04 - accuracy: 1.0000 - val_loss: 3.1312 - val_accuracy: 0.6582\n",
            "Epoch 511/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 2.9771e-04 - accuracy: 1.0000 - val_loss: 3.1324 - val_accuracy: 0.6578\n",
            "Epoch 512/512\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 2.9654e-04 - accuracy: 1.0000 - val_loss: 3.1344 - val_accuracy: 0.6584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Since the capacity of usage is limited, it disconnected our session. Then we reduced the epochs to 40 since we did not see much improvement from that epoch\n",
        "model1 = model.fit(x_train, y_train, batch_size=32, epochs=40, validation_split=0.1)"
      ],
      "metadata": {
        "id": "-o_watGAkAGg",
        "outputId": "4f215174-82d1-4a3b-d532-cb7586bd031d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1407/1407 [==============================] - 44s 31ms/step - loss: 1.7853 - accuracy: 0.3602 - val_loss: 1.6393 - val_accuracy: 0.4168\n",
            "Epoch 2/40\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 1.4619 - accuracy: 0.4820 - val_loss: 1.4850 - val_accuracy: 0.4646\n",
            "Epoch 3/40\n",
            "1407/1407 [==============================] - 41s 29ms/step - loss: 1.3150 - accuracy: 0.5361 - val_loss: 1.3398 - val_accuracy: 0.5336\n",
            "Epoch 4/40\n",
            "1407/1407 [==============================] - 39s 28ms/step - loss: 1.2266 - accuracy: 0.5674 - val_loss: 1.2688 - val_accuracy: 0.5506\n",
            "Epoch 5/40\n",
            "1407/1407 [==============================] - 41s 29ms/step - loss: 1.1573 - accuracy: 0.5931 - val_loss: 1.2015 - val_accuracy: 0.5798\n",
            "Epoch 6/40\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 1.1009 - accuracy: 0.6145 - val_loss: 1.1720 - val_accuracy: 0.5912\n",
            "Epoch 7/40\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 1.0504 - accuracy: 0.6340 - val_loss: 1.1251 - val_accuracy: 0.6104\n",
            "Epoch 8/40\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 1.0082 - accuracy: 0.6506 - val_loss: 1.2931 - val_accuracy: 0.5506\n",
            "Epoch 9/40\n",
            "1407/1407 [==============================] - 40s 29ms/step - loss: 0.9620 - accuracy: 0.6642 - val_loss: 1.1202 - val_accuracy: 0.6088\n",
            "Epoch 10/40\n",
            "1407/1407 [==============================] - 39s 28ms/step - loss: 0.9227 - accuracy: 0.6800 - val_loss: 1.2096 - val_accuracy: 0.5866\n",
            "Epoch 11/40\n",
            "1407/1407 [==============================] - 39s 28ms/step - loss: 0.8834 - accuracy: 0.6950 - val_loss: 1.0490 - val_accuracy: 0.6326\n",
            "Epoch 12/40\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.8445 - accuracy: 0.7091 - val_loss: 1.1618 - val_accuracy: 0.6034\n",
            "Epoch 13/40\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 0.8090 - accuracy: 0.7205 - val_loss: 1.0340 - val_accuracy: 0.6460\n",
            "Epoch 14/40\n",
            "1407/1407 [==============================] - 39s 28ms/step - loss: 0.7710 - accuracy: 0.7331 - val_loss: 1.1394 - val_accuracy: 0.6198\n",
            "Epoch 15/40\n",
            "1407/1407 [==============================] - 40s 29ms/step - loss: 0.7362 - accuracy: 0.7450 - val_loss: 1.0456 - val_accuracy: 0.6492\n",
            "Epoch 16/40\n",
            "1407/1407 [==============================] - 43s 31ms/step - loss: 0.7002 - accuracy: 0.7603 - val_loss: 1.0906 - val_accuracy: 0.6422\n",
            "Epoch 17/40\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 0.6673 - accuracy: 0.7702 - val_loss: 1.0911 - val_accuracy: 0.6378\n",
            "Epoch 18/40\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 0.6337 - accuracy: 0.7852 - val_loss: 1.0749 - val_accuracy: 0.6478\n",
            "Epoch 19/40\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 0.5999 - accuracy: 0.7961 - val_loss: 1.1145 - val_accuracy: 0.6318\n",
            "Epoch 20/40\n",
            "1407/1407 [==============================] - 37s 27ms/step - loss: 0.5675 - accuracy: 0.8093 - val_loss: 1.0987 - val_accuracy: 0.6498\n",
            "Epoch 21/40\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 0.5330 - accuracy: 0.8223 - val_loss: 1.1817 - val_accuracy: 0.6244\n",
            "Epoch 22/40\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 0.5017 - accuracy: 0.8335 - val_loss: 1.2463 - val_accuracy: 0.6250\n",
            "Epoch 23/40\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.4725 - accuracy: 0.8424 - val_loss: 1.3444 - val_accuracy: 0.6052\n",
            "Epoch 24/40\n",
            "1407/1407 [==============================] - 41s 29ms/step - loss: 0.4422 - accuracy: 0.8551 - val_loss: 1.1389 - val_accuracy: 0.6552\n",
            "Epoch 25/40\n",
            "1407/1407 [==============================] - 41s 29ms/step - loss: 0.4112 - accuracy: 0.8668 - val_loss: 1.2186 - val_accuracy: 0.6470\n",
            "Epoch 26/40\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 0.3836 - accuracy: 0.8772 - val_loss: 2.0662 - val_accuracy: 0.5078\n",
            "Epoch 27/40\n",
            "1407/1407 [==============================] - 40s 29ms/step - loss: 0.3566 - accuracy: 0.8862 - val_loss: 1.3072 - val_accuracy: 0.6372\n",
            "Epoch 28/40\n",
            "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3279 - accuracy: 0.8974 - val_loss: 1.3066 - val_accuracy: 0.6352\n",
            "Epoch 29/40\n",
            "1407/1407 [==============================] - 40s 29ms/step - loss: 0.3031 - accuracy: 0.9072 - val_loss: 1.3173 - val_accuracy: 0.6442\n",
            "Epoch 30/40\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.2802 - accuracy: 0.9165 - val_loss: 1.3142 - val_accuracy: 0.6468\n",
            "Epoch 31/40\n",
            "1407/1407 [==============================] - 41s 29ms/step - loss: 0.2551 - accuracy: 0.9264 - val_loss: 1.2722 - val_accuracy: 0.6566\n",
            "Epoch 32/40\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 0.2332 - accuracy: 0.9334 - val_loss: 1.7065 - val_accuracy: 0.6008\n",
            "Epoch 33/40\n",
            "1407/1407 [==============================] - 39s 28ms/step - loss: 0.2104 - accuracy: 0.9438 - val_loss: 1.3630 - val_accuracy: 0.6532\n",
            "Epoch 34/40\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1938 - accuracy: 0.9491 - val_loss: 1.3975 - val_accuracy: 0.6490\n",
            "Epoch 35/40\n",
            "1407/1407 [==============================] - 41s 29ms/step - loss: 0.1725 - accuracy: 0.9580 - val_loss: 1.4477 - val_accuracy: 0.6508\n",
            "Epoch 36/40\n",
            "1407/1407 [==============================] - 39s 28ms/step - loss: 0.1560 - accuracy: 0.9627 - val_loss: 1.4759 - val_accuracy: 0.6518\n",
            "Epoch 37/40\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.1390 - accuracy: 0.9698 - val_loss: 1.5564 - val_accuracy: 0.6338\n",
            "Epoch 38/40\n",
            "1407/1407 [==============================] - 40s 29ms/step - loss: 0.1255 - accuracy: 0.9737 - val_loss: 1.5507 - val_accuracy: 0.6448\n",
            "Epoch 39/40\n",
            "1407/1407 [==============================] - 39s 28ms/step - loss: 0.1114 - accuracy: 0.9782 - val_loss: 1.5555 - val_accuracy: 0.6458\n",
            "Epoch 40/40\n",
            "1407/1407 [==============================] - 42s 30ms/step - loss: 0.0977 - accuracy: 0.9831 - val_loss: 1.8201 - val_accuracy: 0.6190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mrWK5hSB_o"
      },
      "source": [
        "## Defining Deeper Architectures: VGG Models\n",
        "\n",
        "*   Define a deeper model architecture for CIFAR-10 dataset and train the new model for 512 epochs with a batch size of 32. We will use VGG model as the architecture.\n",
        "\n",
        "**Stack two convolutional layers with 32 filters, each of 3 x 3. Use a max pooling layer and next flatten the output of the previous layer and add a dense layer with 128 units before the classification layer. For all the layers, use ReLU activation function. Use same padding for the layers to ensure that the height and width of each layer output matches the input**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A80vLxW9FIek"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgca5dUNSFNc"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Conv2D(32, (3, 3), activation='relu',padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "model2.add(Conv2D(32, (3, 3), activation='relu',padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "model2.add(MaxPooling2D((2, 2)))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model2.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F4czjPGvXAH",
        "outputId": "837be3a7-a46c-4a00-8039-904a19b7357b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 14, 14, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               802944    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 814378 (3.11 MB)\n",
            "Trainable params: 814378 (3.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaPphEBUtlC"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 512 epochs with a batch size of 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc2qtU0mUvVA"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "model2.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2_data = model2.fit(x_train, y_train, batch_size=32, epochs=40, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScpkKq2hv1q8",
        "outputId": "a5ec2d70-771a-4e5f-f1a4-a1ff151bc87f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1407/1407 [==============================] - 117s 82ms/step - loss: 1.6917 - accuracy: 0.3943 - val_loss: 1.5724 - val_accuracy: 0.4372\n",
            "Epoch 2/40\n",
            "1407/1407 [==============================] - 113s 80ms/step - loss: 1.3321 - accuracy: 0.5280 - val_loss: 1.3064 - val_accuracy: 0.5398\n",
            "Epoch 3/40\n",
            "1407/1407 [==============================] - 113s 80ms/step - loss: 1.1875 - accuracy: 0.5825 - val_loss: 1.2394 - val_accuracy: 0.5594\n",
            "Epoch 4/40\n",
            "1407/1407 [==============================] - 113s 80ms/step - loss: 1.0785 - accuracy: 0.6224 - val_loss: 1.1311 - val_accuracy: 0.6092\n",
            "Epoch 5/40\n",
            "1407/1407 [==============================] - 113s 81ms/step - loss: 0.9859 - accuracy: 0.6574 - val_loss: 1.1929 - val_accuracy: 0.5696\n",
            "Epoch 6/40\n",
            "1407/1407 [==============================] - 115s 82ms/step - loss: 0.9016 - accuracy: 0.6888 - val_loss: 1.0566 - val_accuracy: 0.6342\n",
            "Epoch 7/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.8266 - accuracy: 0.7139 - val_loss: 1.1269 - val_accuracy: 0.6152\n",
            "Epoch 8/40\n",
            "1407/1407 [==============================] - 113s 80ms/step - loss: 0.7564 - accuracy: 0.7403 - val_loss: 1.2741 - val_accuracy: 0.5792\n",
            "Epoch 9/40\n",
            "1407/1407 [==============================] - 113s 80ms/step - loss: 0.6882 - accuracy: 0.7636 - val_loss: 1.1468 - val_accuracy: 0.6348\n",
            "Epoch 10/40\n",
            "1407/1407 [==============================] - 112s 80ms/step - loss: 0.6190 - accuracy: 0.7875 - val_loss: 1.0093 - val_accuracy: 0.6788\n",
            "Epoch 11/40\n",
            "1407/1407 [==============================] - 113s 80ms/step - loss: 0.5507 - accuracy: 0.8124 - val_loss: 1.0299 - val_accuracy: 0.6702\n",
            "Epoch 12/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.4863 - accuracy: 0.8342 - val_loss: 1.1114 - val_accuracy: 0.6642\n",
            "Epoch 13/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.4228 - accuracy: 0.8561 - val_loss: 1.1449 - val_accuracy: 0.6634\n",
            "Epoch 14/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.3604 - accuracy: 0.8800 - val_loss: 1.3417 - val_accuracy: 0.6284\n",
            "Epoch 15/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.3003 - accuracy: 0.9012 - val_loss: 1.2689 - val_accuracy: 0.6588\n",
            "Epoch 16/40\n",
            "1407/1407 [==============================] - 113s 80ms/step - loss: 0.2468 - accuracy: 0.9204 - val_loss: 1.3653 - val_accuracy: 0.6466\n",
            "Epoch 17/40\n",
            "1407/1407 [==============================] - 117s 83ms/step - loss: 0.1976 - accuracy: 0.9393 - val_loss: 1.6090 - val_accuracy: 0.6298\n",
            "Epoch 18/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.1557 - accuracy: 0.9526 - val_loss: 1.4731 - val_accuracy: 0.6658\n",
            "Epoch 19/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.1154 - accuracy: 0.9685 - val_loss: 1.6914 - val_accuracy: 0.6474\n",
            "Epoch 20/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.0867 - accuracy: 0.9778 - val_loss: 1.6989 - val_accuracy: 0.6612\n",
            "Epoch 21/40\n",
            "1407/1407 [==============================] - 111s 79ms/step - loss: 0.0633 - accuracy: 0.9857 - val_loss: 1.7855 - val_accuracy: 0.6642\n",
            "Epoch 22/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.0390 - accuracy: 0.9937 - val_loss: 1.8391 - val_accuracy: 0.6680\n",
            "Epoch 23/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.0215 - accuracy: 0.9981 - val_loss: 1.9107 - val_accuracy: 0.6728\n",
            "Epoch 24/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.0132 - accuracy: 0.9992 - val_loss: 1.9957 - val_accuracy: 0.6732\n",
            "Epoch 25/40\n",
            "1407/1407 [==============================] - 113s 80ms/step - loss: 0.0088 - accuracy: 0.9998 - val_loss: 2.0611 - val_accuracy: 0.6730\n",
            "Epoch 26/40\n",
            "1407/1407 [==============================] - 111s 79ms/step - loss: 0.0068 - accuracy: 0.9998 - val_loss: 2.1098 - val_accuracy: 0.6736\n",
            "Epoch 27/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.0055 - accuracy: 0.9999 - val_loss: 2.1618 - val_accuracy: 0.6746\n",
            "Epoch 28/40\n",
            "1407/1407 [==============================] - 113s 81ms/step - loss: 0.0046 - accuracy: 0.9999 - val_loss: 2.2134 - val_accuracy: 0.6718\n",
            "Epoch 29/40\n",
            "1407/1407 [==============================] - 112s 80ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.2449 - val_accuracy: 0.6714\n",
            "Epoch 30/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.0036 - accuracy: 0.9999 - val_loss: 2.2780 - val_accuracy: 0.6700\n",
            "Epoch 31/40\n",
            "1407/1407 [==============================] - 116s 83ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.3154 - val_accuracy: 0.6704\n",
            "Epoch 32/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.3413 - val_accuracy: 0.6710\n",
            "Epoch 33/40\n",
            "1407/1407 [==============================] - 111s 79ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.3639 - val_accuracy: 0.6688\n",
            "Epoch 34/40\n",
            "1407/1407 [==============================] - 111s 79ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.3889 - val_accuracy: 0.6700\n",
            "Epoch 35/40\n",
            "1407/1407 [==============================] - 111s 79ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.4099 - val_accuracy: 0.6702\n",
            "Epoch 36/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.4347 - val_accuracy: 0.6696\n",
            "Epoch 37/40\n",
            "1407/1407 [==============================] - 112s 79ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.4529 - val_accuracy: 0.6704\n",
            "Epoch 38/40\n",
            "1407/1407 [==============================] - 116s 82ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.4787 - val_accuracy: 0.6700\n",
            "Epoch 39/40\n",
            "1407/1407 [==============================] - 111s 79ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.4917 - val_accuracy: 0.6702\n",
            "Epoch 40/40\n",
            "1407/1407 [==============================] - 114s 81ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.5113 - val_accuracy: 0.6678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2cRr2ZFSFds"
      },
      "source": [
        "*   Compare the performance of both the models by plotting the loss and accuracy curves of both the training steps. Does the deeper model perform better? Comment on the observation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8OSHAf5SJPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe39a5b-0694-4e8d-9a52-c8766a8be9ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "source": [
        "# solution\n",
        "print(model2_data.history.keys())\n",
        "print(model1.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(211)\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(model2_data.history['loss'], color='blue', label='train')\n",
        "plt.plot(model2_data.history['val_loss'], color='red', label='val')\n",
        "plt.plot(model1.history['loss'], color='green', label='train')\n",
        "plt.plot(model1.history['val_loss'], color='yellow', label='val')\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.title('Classification Accuracy')\n",
        "plt.plot(model2_data.history['accuracy'], color='blue', label='train')\n",
        "plt.plot(model2_data.history['val_accuracy'], color='red', label='val')\n",
        "plt.plot(model1.history['accuracy'], color='green', label='train')\n",
        "plt.plot(model1.history['val_accuracy'], color='yellow', label='val')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "3Taxs1jY7dQq",
        "outputId": "d5463e40-2a95-4137-9bbe-4620e1848b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri9kU3wa3Rei"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzXmO1WoSKMY"
      },
      "source": [
        "*   Use predict function to predict the output for the test split\n",
        "*   Plot the confusion matrix for the new model and comment on the class confusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DObaoxhaSMUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6078cee2-ca82-40d3-f937-6e0a12f3cbe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 8s 26ms/step\n",
            "(10000, 10)\n"
          ]
        }
      ],
      "source": [
        "# solution\n",
        "import numpy as np\n",
        "predictions = model2.predict(x_test)\n",
        "\n",
        "print(predictions.shape)\n",
        "predictions = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "gt = np.argmax(y_test, axis=1)\n",
        "confusion_matrix(gt, predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chc-0yq-8qEB",
        "outputId": "f43518e0-f70f-4407-9ede-1eceff271bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[723,  24,  52,  18,  17,  11,  14,   8,  89,  44],\n",
              "       [ 35, 772,  15,  14,   8,   4,  13,   4,  39,  96],\n",
              "       [ 65,   8, 536,  67,  97,  81,  77,  35,  19,  15],\n",
              "       [ 36,  16,  82, 460,  56, 211,  75,  31,  17,  16],\n",
              "       [ 28,   5,  86,  59, 618,  47,  49,  82,  20,   6],\n",
              "       [ 22,   2,  75, 192,  48, 569,  34,  45,   7,   6],\n",
              "       [  7,  14,  57,  66,  50,  45, 736,   8,  11,   6],\n",
              "       [ 28,   6,  37,  44,  70,  79,   7, 708,   5,  16],\n",
              "       [ 69,  44,  15,  12,  12,  15,   8,   3, 784,  38],\n",
              "       [ 41, 112,   8,  21,  10,  13,  12,  27,  42, 714]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBrvRomU5O_"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffwVz-FLSNG7"
      },
      "source": [
        "*    Print the test accuracy for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4WX3_uLSN5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd11192c-94b4-4586-f8aa-45c936a0bef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 6s 19ms/step - loss: 2.5742 - accuracy: 0.6620\n",
            "Test loss: 2.5741758346557617\n",
            "Test accuracy: 0.6620000004768372\n"
          ]
        }
      ],
      "source": [
        "# solution\n",
        "test_loss, test_acc = model2.evaluate(x_test, y_test)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dySqfA6PVBjQ"
      },
      "source": [
        "## Define the complete VGG architecture.\n",
        "\n",
        "**Stack two convolutional layers with 64 filters, each of 3 x 3 followed by max pooling layer. Stack two more convolutional layers with 128 filters, each of 3 x 3, followed by max pooling, follwed by two more convolutional layers with 256 filters, each of 3 x 3, followed by max pooling. Flatten the output of the previous layer and add a dense layer with 128 units before the classification layer. For all the layers, use ReLU activation function. Use same padding for the layers to ensure that the height and width of each layer output matches the input**\n",
        "\n",
        "*   Change the size of input to 64 x 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm35siILFNT0"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH4lDVBuVA_Q"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "model3 = Sequential()\n",
        "\n",
        "model3.add(Conv2D(64, (3, 3), activation='relu',padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "model3.add(Conv2D(64, (3, 3), activation='relu', padding='same',kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "model3.add(MaxPooling2D((2, 2)))\n",
        "model3.add(Conv2D(128, (3, 3), activation='relu',padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "model3.add(Conv2D(128, (3, 3), activation='relu',padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "model3.add(MaxPooling2D((2, 2)))\n",
        "model3.add(Conv2D(256, (3, 3), activation='relu', padding='same',kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "model3.add(Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "model3.add(MaxPooling2D((2, 2)))\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model3.add(Dense(10, activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbp0YLjY-ToI",
        "outputId": "df9c7337-9ae8-49ca-ddc5-23ea33ab628f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 16, 16, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               524416    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1671114 (6.37 MB)\n",
            "Trainable params: 1671114 (6.37 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu_B8kJGWhcM"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 100 epochs with a batch size of 32.\n",
        "*   Predict the output for the test split and plot the confusion matrix for the new model and comment on the class confusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4elnDWnjEbmO"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "model3.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3_data = model3.fit(x_train, y_train, batch_size=32, epochs=40, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D-GxWt--_As",
        "outputId": "88fc68b7-f759-43db-fe34-6f0b7b1a4968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1407/1407 [==============================] - 29s 10ms/step - loss: 3.3342 - accuracy: 0.1245 - val_loss: 3.6185 - val_accuracy: 0.1206\n",
            "Epoch 2/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 3.5779 - accuracy: 0.1169 - val_loss: 3.6092 - val_accuracy: 0.1040\n",
            "Epoch 3/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 3.5679 - accuracy: 0.1332 - val_loss: 3.6325 - val_accuracy: 0.1064\n",
            "Epoch 4/40\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 3.5767 - accuracy: 0.1184 - val_loss: 3.6053 - val_accuracy: 0.1476\n",
            "Epoch 5/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 3.5576 - accuracy: 0.1351 - val_loss: 3.5749 - val_accuracy: 0.1434\n",
            "Epoch 6/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 3.5752 - accuracy: 0.1188 - val_loss: 3.5883 - val_accuracy: 0.1292\n",
            "Epoch 7/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 3.5380 - accuracy: 0.1473 - val_loss: 3.5622 - val_accuracy: 0.1524\n",
            "Epoch 8/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 3.5221 - accuracy: 0.1503 - val_loss: 3.5548 - val_accuracy: 0.1480\n",
            "Epoch 9/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 3.5137 - accuracy: 0.1486 - val_loss: 3.5465 - val_accuracy: 0.1678\n",
            "Epoch 10/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.3752 - accuracy: 0.1731 - val_loss: 2.0511 - val_accuracy: 0.2240\n",
            "Epoch 11/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.8155 - accuracy: 0.1325 - val_loss: 3.6403 - val_accuracy: 0.1024\n",
            "Epoch 12/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 3.6128 - accuracy: 0.0997 - val_loss: 3.5509 - val_accuracy: 0.1024\n",
            "Epoch 13/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 3.5855 - accuracy: 0.1108 - val_loss: 3.5451 - val_accuracy: 0.1258\n",
            "Epoch 14/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 3.5790 - accuracy: 0.1492 - val_loss: 3.5370 - val_accuracy: 0.1656\n",
            "Epoch 15/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 3.5682 - accuracy: 0.1544 - val_loss: 3.5223 - val_accuracy: 0.1614\n",
            "Epoch 16/40\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 3.5438 - accuracy: 0.1604 - val_loss: 3.4901 - val_accuracy: 0.1506\n",
            "Epoch 17/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.9502 - accuracy: 0.1493 - val_loss: 2.2414 - val_accuracy: 0.1572\n",
            "Epoch 18/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.2222 - accuracy: 0.1636 - val_loss: 2.2092 - val_accuracy: 0.1630\n",
            "Epoch 19/40\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 2.2042 - accuracy: 0.1662 - val_loss: 2.1982 - val_accuracy: 0.1694\n",
            "Epoch 20/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.1799 - accuracy: 0.1797 - val_loss: 2.1581 - val_accuracy: 0.1992\n",
            "Epoch 21/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.1956 - accuracy: 0.1621 - val_loss: 2.1742 - val_accuracy: 0.1506\n",
            "Epoch 22/40\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 2.1169 - accuracy: 0.1797 - val_loss: 2.1567 - val_accuracy: 0.1654\n",
            "Epoch 23/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.1357 - accuracy: 0.1681 - val_loss: 2.2710 - val_accuracy: 0.1120\n",
            "Epoch 24/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.2357 - accuracy: 0.1575 - val_loss: 2.1901 - val_accuracy: 0.1844\n",
            "Epoch 25/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.1600 - accuracy: 0.1715 - val_loss: 2.2785 - val_accuracy: 0.1218\n",
            "Epoch 26/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.1447 - accuracy: 0.1898 - val_loss: 2.0313 - val_accuracy: 0.2692\n",
            "Epoch 27/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.1910 - accuracy: 0.1925 - val_loss: 2.3432 - val_accuracy: 0.1024\n",
            "Epoch 28/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.2721 - accuracy: 0.1286 - val_loss: 2.2159 - val_accuracy: 0.1714\n",
            "Epoch 29/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.1952 - accuracy: 0.1737 - val_loss: 2.1720 - val_accuracy: 0.1812\n",
            "Epoch 30/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.2787 - accuracy: 0.1128 - val_loss: 2.2376 - val_accuracy: 0.1676\n",
            "Epoch 31/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.2077 - accuracy: 0.1800 - val_loss: 2.1895 - val_accuracy: 0.1886\n",
            "Epoch 32/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.1750 - accuracy: 0.1857 - val_loss: 2.1728 - val_accuracy: 0.1978\n",
            "Epoch 33/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.1791 - accuracy: 0.1870 - val_loss: 2.3381 - val_accuracy: 0.0976\n",
            "Epoch 34/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.2966 - accuracy: 0.1031 - val_loss: 2.2800 - val_accuracy: 0.1194\n",
            "Epoch 35/40\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 2.2707 - accuracy: 0.1476 - val_loss: 2.2628 - val_accuracy: 0.1728\n",
            "Epoch 36/40\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 2.2498 - accuracy: 0.1732 - val_loss: 2.2380 - val_accuracy: 0.1806\n",
            "Epoch 37/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.2210 - accuracy: 0.1789 - val_loss: 2.2084 - val_accuracy: 0.1754\n",
            "Epoch 38/40\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 2.1994 - accuracy: 0.1759 - val_loss: 2.1992 - val_accuracy: 0.1690\n",
            "Epoch 39/40\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 2.1882 - accuracy: 0.1798 - val_loss: 2.1899 - val_accuracy: 0.1814\n",
            "Epoch 40/40\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.1758 - accuracy: 0.1837 - val_loss: 2.1786 - val_accuracy: 0.1724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model3.predict(x_test)\n",
        "\n",
        "print(pred.shape)\n",
        "pred = np.argmax(pred, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riT2CG5L_SZw",
        "outputId": "9d22c37c-927d-49c8-a6fa-f949856f2b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 4ms/step\n",
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "gt = np.argmax(y_test, axis=1)\n",
        "\n",
        "confusion_matrix(gt, pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_4nkOlo_44o",
        "outputId": "4b5a93bd-6464-44ce-be46-3e769e0bfa31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2,   1,   0,   0,   5,   0, 580,   0, 202, 210],\n",
              "       [  5,   1,   0,   6,  13,   0, 305,   0, 376, 294],\n",
              "       [ 11,   1,   0,   8,  24,   4, 714,   0, 129, 109],\n",
              "       [  8,   2,   0,  31,  51,  11, 643,   0,  94, 160],\n",
              "       [  3,   1,   0,  14,  69,   1, 665,   0, 124, 123],\n",
              "       [  8,   3,   0,  28,  37,   6, 697,   0, 116, 105],\n",
              "       [  8,   3,   0,  11,  25,   3, 700,   0, 151,  99],\n",
              "       [  9,   1,   0,   5,  40,   2, 452,   0, 160, 331],\n",
              "       [  2,   0,   0,   4,   7,   0, 281,   0, 323, 383],\n",
              "       [  1,   0,   0,   1,  12,   0, 197,   0, 202, 587]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dlzFt0SXGDQ"
      },
      "source": [
        "# Understanding deep networks\n",
        "\n",
        "*   What is the use of activation functions in network? Why is it needed?\n",
        "*   We have used softmax activation function in the exercise. There are other activation functions available too. What is the difference between sigmoid activation and softmax activation?\n",
        "*   What is the difference between categorical crossentropy and binary crossentropy loss?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPy_1EWXX6fp"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7JtHCmIxSmn"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "It is not always necessary to train models from scratch. We can use the knowledge of networks trained on other tasks to learn the task at hand. In this exercise, we will explore the use of pre-trained weights and train on the CIFAR-10 dataset.\n",
        "\n",
        "*   Create a base imagenet pretrained InceptionV3 model.\n",
        "    *    Hint: Use tf.keras.applications to create the model\n",
        "    *    Pay attention to the include_top parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "PvJ-OYIt6FRp"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "-ZPNmumpxJMo",
        "outputId": "8d30a155-0ddb-469a-f571-ca56bbbf546c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 127, 127, 32)         864       ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 127, 127, 32)         96        ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 127, 127, 32)         0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 32)         9216      ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 125, 125, 32)         96        ['conv2d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 125, 125, 32)         0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 125, 125, 64)         18432     ['activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 125, 125, 64)         192       ['conv2d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 125, 125, 64)         0         ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 62, 62, 64)           0         ['activation_2[0][0]']        \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 62, 62, 80)           5120      ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 62, 62, 80)           240       ['conv2d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 62, 62, 80)           0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 60, 60, 192)          138240    ['activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 60, 60, 192)          576       ['conv2d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 60, 60, 192)          0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 29, 29, 192)          0         ['activation_4[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 29, 29, 64)           12288     ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 29, 29, 64)           192       ['conv2d_8[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, 29, 29, 64)           0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 29, 29, 48)           9216      ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 29, 29, 96)           55296     ['activation_8[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 29, 29, 48)           144       ['conv2d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 29, 29, 96)           288       ['conv2d_9[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 29, 29, 48)           0         ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, 29, 29, 96)           0         ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " average_pooling2d (Average  (None, 29, 29, 192)          0         ['max_pooling2d_1[0][0]']     \n",
            " Pooling2D)                                                                                       \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 29, 29, 64)           12288     ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 29, 29, 64)           76800     ['activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 29, 29, 96)           82944     ['activation_9[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 29, 29, 32)           6144      ['average_pooling2d[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 29, 29, 64)           192       ['conv2d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 29, 29, 64)           192       ['conv2d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, 29, 29, 96)           288       ['conv2d_10[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, 29, 29, 32)           96        ['conv2d_11[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 29, 29, 64)           0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, 29, 29, 64)           0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 29, 29, 96)           0         ['batch_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 29, 29, 32)           0         ['batch_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed0 (Concatenate)        (None, 29, 29, 256)          0         ['activation_5[0][0]',        \n",
            "                                                                     'activation_7[0][0]',        \n",
            "                                                                     'activation_10[0][0]',       \n",
            "                                                                     'activation_11[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 29, 29, 64)           16384     ['mixed0[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, 29, 29, 64)           192       ['conv2d_15[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 29, 29, 64)           0         ['batch_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 29, 29, 48)           12288     ['mixed0[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 29, 29, 96)           55296     ['activation_15[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, 29, 29, 48)           144       ['conv2d_13[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, 29, 29, 96)           288       ['conv2d_16[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 29, 29, 48)           0         ['batch_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 29, 29, 96)           0         ['batch_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (Avera  (None, 29, 29, 256)          0         ['mixed0[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 29, 29, 64)           16384     ['mixed0[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 29, 29, 64)           76800     ['activation_13[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 29, 29, 96)           82944     ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 29, 29, 64)           16384     ['average_pooling2d_1[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, 29, 29, 64)           192       ['conv2d_12[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, 29, 29, 64)           192       ['conv2d_14[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_17 (Ba  (None, 29, 29, 96)           288       ['conv2d_17[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_18 (Ba  (None, 29, 29, 64)           192       ['conv2d_18[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 29, 29, 64)           0         ['batch_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 29, 29, 64)           0         ['batch_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 29, 29, 96)           0         ['batch_normalization_17[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_18 (Activation)  (None, 29, 29, 64)           0         ['batch_normalization_18[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed1 (Concatenate)        (None, 29, 29, 288)          0         ['activation_12[0][0]',       \n",
            "                                                                     'activation_14[0][0]',       \n",
            "                                                                     'activation_17[0][0]',       \n",
            "                                                                     'activation_18[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)          (None, 29, 29, 64)           18432     ['mixed1[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_22 (Ba  (None, 29, 29, 64)           192       ['conv2d_22[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_22 (Activation)  (None, 29, 29, 64)           0         ['batch_normalization_22[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)          (None, 29, 29, 48)           13824     ['mixed1[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)          (None, 29, 29, 96)           55296     ['activation_22[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_20 (Ba  (None, 29, 29, 48)           144       ['conv2d_20[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_23 (Ba  (None, 29, 29, 96)           288       ['conv2d_23[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_20 (Activation)  (None, 29, 29, 48)           0         ['batch_normalization_20[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_23 (Activation)  (None, 29, 29, 96)           0         ['batch_normalization_23[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (Avera  (None, 29, 29, 288)          0         ['mixed1[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)          (None, 29, 29, 64)           18432     ['mixed1[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)          (None, 29, 29, 64)           76800     ['activation_20[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)          (None, 29, 29, 96)           82944     ['activation_23[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)          (None, 29, 29, 64)           18432     ['average_pooling2d_2[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_19 (Ba  (None, 29, 29, 64)           192       ['conv2d_19[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_21 (Ba  (None, 29, 29, 64)           192       ['conv2d_21[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_24 (Ba  (None, 29, 29, 96)           288       ['conv2d_24[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_25 (Ba  (None, 29, 29, 64)           192       ['conv2d_25[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_19 (Activation)  (None, 29, 29, 64)           0         ['batch_normalization_19[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_21 (Activation)  (None, 29, 29, 64)           0         ['batch_normalization_21[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_24 (Activation)  (None, 29, 29, 96)           0         ['batch_normalization_24[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_25 (Activation)  (None, 29, 29, 64)           0         ['batch_normalization_25[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed2 (Concatenate)        (None, 29, 29, 288)          0         ['activation_19[0][0]',       \n",
            "                                                                     'activation_21[0][0]',       \n",
            "                                                                     'activation_24[0][0]',       \n",
            "                                                                     'activation_25[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)          (None, 29, 29, 64)           18432     ['mixed2[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_27 (Ba  (None, 29, 29, 64)           192       ['conv2d_27[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_27 (Activation)  (None, 29, 29, 64)           0         ['batch_normalization_27[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)          (None, 29, 29, 96)           55296     ['activation_27[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_28 (Ba  (None, 29, 29, 96)           288       ['conv2d_28[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_28 (Activation)  (None, 29, 29, 96)           0         ['batch_normalization_28[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)          (None, 14, 14, 384)          995328    ['mixed2[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)          (None, 14, 14, 96)           82944     ['activation_28[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_26 (Ba  (None, 14, 14, 384)          1152      ['conv2d_26[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_29 (Ba  (None, 14, 14, 96)           288       ['conv2d_29[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_26 (Activation)  (None, 14, 14, 384)          0         ['batch_normalization_26[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_29 (Activation)  (None, 14, 14, 96)           0         ['batch_normalization_29[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, 14, 14, 288)          0         ['mixed2[0][0]']              \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " mixed3 (Concatenate)        (None, 14, 14, 768)          0         ['activation_26[0][0]',       \n",
            "                                                                     'activation_29[0][0]',       \n",
            "                                                                     'max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)          (None, 14, 14, 128)          98304     ['mixed3[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_34 (Ba  (None, 14, 14, 128)          384       ['conv2d_34[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_34 (Activation)  (None, 14, 14, 128)          0         ['batch_normalization_34[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)          (None, 14, 14, 128)          114688    ['activation_34[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_35 (Ba  (None, 14, 14, 128)          384       ['conv2d_35[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_35 (Activation)  (None, 14, 14, 128)          0         ['batch_normalization_35[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)          (None, 14, 14, 128)          98304     ['mixed3[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)          (None, 14, 14, 128)          114688    ['activation_35[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_31 (Ba  (None, 14, 14, 128)          384       ['conv2d_31[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_36 (Ba  (None, 14, 14, 128)          384       ['conv2d_36[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_31 (Activation)  (None, 14, 14, 128)          0         ['batch_normalization_31[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_36 (Activation)  (None, 14, 14, 128)          0         ['batch_normalization_36[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)          (None, 14, 14, 128)          114688    ['activation_31[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)          (None, 14, 14, 128)          114688    ['activation_36[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_32 (Ba  (None, 14, 14, 128)          384       ['conv2d_32[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_37 (Ba  (None, 14, 14, 128)          384       ['conv2d_37[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_32 (Activation)  (None, 14, 14, 128)          0         ['batch_normalization_32[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_37 (Activation)  (None, 14, 14, 128)          0         ['batch_normalization_37[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (Avera  (None, 14, 14, 768)          0         ['mixed3[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)          (None, 14, 14, 192)          147456    ['mixed3[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)          (None, 14, 14, 192)          172032    ['activation_32[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)          (None, 14, 14, 192)          172032    ['activation_37[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)          (None, 14, 14, 192)          147456    ['average_pooling2d_3[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_30 (Ba  (None, 14, 14, 192)          576       ['conv2d_30[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_33 (Ba  (None, 14, 14, 192)          576       ['conv2d_33[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_38 (Ba  (None, 14, 14, 192)          576       ['conv2d_38[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_39 (Ba  (None, 14, 14, 192)          576       ['conv2d_39[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_30 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_30[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_33 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_33[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_38 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_38[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_39 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_39[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed4 (Concatenate)        (None, 14, 14, 768)          0         ['activation_30[0][0]',       \n",
            "                                                                     'activation_33[0][0]',       \n",
            "                                                                     'activation_38[0][0]',       \n",
            "                                                                     'activation_39[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)          (None, 14, 14, 160)          122880    ['mixed4[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_44 (Ba  (None, 14, 14, 160)          480       ['conv2d_44[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_44 (Activation)  (None, 14, 14, 160)          0         ['batch_normalization_44[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)          (None, 14, 14, 160)          179200    ['activation_44[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_45 (Ba  (None, 14, 14, 160)          480       ['conv2d_45[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_45 (Activation)  (None, 14, 14, 160)          0         ['batch_normalization_45[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)          (None, 14, 14, 160)          122880    ['mixed4[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)          (None, 14, 14, 160)          179200    ['activation_45[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_41 (Ba  (None, 14, 14, 160)          480       ['conv2d_41[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_46 (Ba  (None, 14, 14, 160)          480       ['conv2d_46[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_41 (Activation)  (None, 14, 14, 160)          0         ['batch_normalization_41[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_46 (Activation)  (None, 14, 14, 160)          0         ['batch_normalization_46[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)          (None, 14, 14, 160)          179200    ['activation_41[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)          (None, 14, 14, 160)          179200    ['activation_46[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_42 (Ba  (None, 14, 14, 160)          480       ['conv2d_42[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_47 (Ba  (None, 14, 14, 160)          480       ['conv2d_47[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_42 (Activation)  (None, 14, 14, 160)          0         ['batch_normalization_42[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_47 (Activation)  (None, 14, 14, 160)          0         ['batch_normalization_47[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_4 (Avera  (None, 14, 14, 768)          0         ['mixed4[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)          (None, 14, 14, 192)          147456    ['mixed4[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)          (None, 14, 14, 192)          215040    ['activation_42[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)          (None, 14, 14, 192)          215040    ['activation_47[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)          (None, 14, 14, 192)          147456    ['average_pooling2d_4[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_40 (Ba  (None, 14, 14, 192)          576       ['conv2d_40[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_43 (Ba  (None, 14, 14, 192)          576       ['conv2d_43[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_48 (Ba  (None, 14, 14, 192)          576       ['conv2d_48[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_49 (Ba  (None, 14, 14, 192)          576       ['conv2d_49[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_40 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_40[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_43 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_43[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_48 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_48[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_49 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_49[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed5 (Concatenate)        (None, 14, 14, 768)          0         ['activation_40[0][0]',       \n",
            "                                                                     'activation_43[0][0]',       \n",
            "                                                                     'activation_48[0][0]',       \n",
            "                                                                     'activation_49[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)          (None, 14, 14, 160)          122880    ['mixed5[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_54 (Ba  (None, 14, 14, 160)          480       ['conv2d_54[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_54 (Activation)  (None, 14, 14, 160)          0         ['batch_normalization_54[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)          (None, 14, 14, 160)          179200    ['activation_54[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_55 (Ba  (None, 14, 14, 160)          480       ['conv2d_55[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_55 (Activation)  (None, 14, 14, 160)          0         ['batch_normalization_55[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)          (None, 14, 14, 160)          122880    ['mixed5[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)          (None, 14, 14, 160)          179200    ['activation_55[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_51 (Ba  (None, 14, 14, 160)          480       ['conv2d_51[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_56 (Ba  (None, 14, 14, 160)          480       ['conv2d_56[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_51 (Activation)  (None, 14, 14, 160)          0         ['batch_normalization_51[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_56 (Activation)  (None, 14, 14, 160)          0         ['batch_normalization_56[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)          (None, 14, 14, 160)          179200    ['activation_51[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)          (None, 14, 14, 160)          179200    ['activation_56[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_52 (Ba  (None, 14, 14, 160)          480       ['conv2d_52[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_57 (Ba  (None, 14, 14, 160)          480       ['conv2d_57[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_52 (Activation)  (None, 14, 14, 160)          0         ['batch_normalization_52[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_57 (Activation)  (None, 14, 14, 160)          0         ['batch_normalization_57[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_5 (Avera  (None, 14, 14, 768)          0         ['mixed5[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)          (None, 14, 14, 192)          147456    ['mixed5[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)          (None, 14, 14, 192)          215040    ['activation_52[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)          (None, 14, 14, 192)          215040    ['activation_57[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)          (None, 14, 14, 192)          147456    ['average_pooling2d_5[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_50 (Ba  (None, 14, 14, 192)          576       ['conv2d_50[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_53 (Ba  (None, 14, 14, 192)          576       ['conv2d_53[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_58 (Ba  (None, 14, 14, 192)          576       ['conv2d_58[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_59 (Ba  (None, 14, 14, 192)          576       ['conv2d_59[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_50 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_50[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_53 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_53[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_58 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_58[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_59 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_59[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed6 (Concatenate)        (None, 14, 14, 768)          0         ['activation_50[0][0]',       \n",
            "                                                                     'activation_53[0][0]',       \n",
            "                                                                     'activation_58[0][0]',       \n",
            "                                                                     'activation_59[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)          (None, 14, 14, 192)          147456    ['mixed6[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_64 (Ba  (None, 14, 14, 192)          576       ['conv2d_64[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_64 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_64[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)          (None, 14, 14, 192)          258048    ['activation_64[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_65 (Ba  (None, 14, 14, 192)          576       ['conv2d_65[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_65 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_65[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)          (None, 14, 14, 192)          147456    ['mixed6[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)          (None, 14, 14, 192)          258048    ['activation_65[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_61 (Ba  (None, 14, 14, 192)          576       ['conv2d_61[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_66 (Ba  (None, 14, 14, 192)          576       ['conv2d_66[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_61 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_61[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_66 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_66[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)          (None, 14, 14, 192)          258048    ['activation_61[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)          (None, 14, 14, 192)          258048    ['activation_66[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_62 (Ba  (None, 14, 14, 192)          576       ['conv2d_62[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_67 (Ba  (None, 14, 14, 192)          576       ['conv2d_67[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_62 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_62[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_67 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_67[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_6 (Avera  (None, 14, 14, 768)          0         ['mixed6[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)          (None, 14, 14, 192)          147456    ['mixed6[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)          (None, 14, 14, 192)          258048    ['activation_62[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)          (None, 14, 14, 192)          258048    ['activation_67[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)          (None, 14, 14, 192)          147456    ['average_pooling2d_6[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_60 (Ba  (None, 14, 14, 192)          576       ['conv2d_60[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_63 (Ba  (None, 14, 14, 192)          576       ['conv2d_63[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_68 (Ba  (None, 14, 14, 192)          576       ['conv2d_68[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_69 (Ba  (None, 14, 14, 192)          576       ['conv2d_69[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_60 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_60[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_63 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_63[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_68 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_68[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_69 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_69[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed7 (Concatenate)        (None, 14, 14, 768)          0         ['activation_60[0][0]',       \n",
            "                                                                     'activation_63[0][0]',       \n",
            "                                                                     'activation_68[0][0]',       \n",
            "                                                                     'activation_69[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)          (None, 14, 14, 192)          147456    ['mixed7[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_72 (Ba  (None, 14, 14, 192)          576       ['conv2d_72[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_72 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_72[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)          (None, 14, 14, 192)          258048    ['activation_72[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_73 (Ba  (None, 14, 14, 192)          576       ['conv2d_73[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_73 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_73[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)          (None, 14, 14, 192)          147456    ['mixed7[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)          (None, 14, 14, 192)          258048    ['activation_73[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_70 (Ba  (None, 14, 14, 192)          576       ['conv2d_70[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_74 (Ba  (None, 14, 14, 192)          576       ['conv2d_74[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_70 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_70[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_74 (Activation)  (None, 14, 14, 192)          0         ['batch_normalization_74[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)          (None, 6, 6, 320)            552960    ['activation_70[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)          (None, 6, 6, 192)            331776    ['activation_74[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_71 (Ba  (None, 6, 6, 320)            960       ['conv2d_71[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_75 (Ba  (None, 6, 6, 192)            576       ['conv2d_75[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_71 (Activation)  (None, 6, 6, 320)            0         ['batch_normalization_71[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_75 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_75[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 768)            0         ['mixed7[0][0]']              \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " mixed8 (Concatenate)        (None, 6, 6, 1280)           0         ['activation_71[0][0]',       \n",
            "                                                                     'activation_75[0][0]',       \n",
            "                                                                     'max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)          (None, 6, 6, 448)            573440    ['mixed8[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_80 (Ba  (None, 6, 6, 448)            1344      ['conv2d_80[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_80 (Activation)  (None, 6, 6, 448)            0         ['batch_normalization_80[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)          (None, 6, 6, 384)            491520    ['mixed8[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)          (None, 6, 6, 384)            1548288   ['activation_80[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_77 (Ba  (None, 6, 6, 384)            1152      ['conv2d_77[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_81 (Ba  (None, 6, 6, 384)            1152      ['conv2d_81[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_77 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_77[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_81 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_81[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)          (None, 6, 6, 384)            442368    ['activation_77[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)          (None, 6, 6, 384)            442368    ['activation_77[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)          (None, 6, 6, 384)            442368    ['activation_81[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)          (None, 6, 6, 384)            442368    ['activation_81[0][0]']       \n",
            "                                                                                                  \n",
            " average_pooling2d_7 (Avera  (None, 6, 6, 1280)           0         ['mixed8[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)          (None, 6, 6, 320)            409600    ['mixed8[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_78 (Ba  (None, 6, 6, 384)            1152      ['conv2d_78[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_79 (Ba  (None, 6, 6, 384)            1152      ['conv2d_79[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_82 (Ba  (None, 6, 6, 384)            1152      ['conv2d_82[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_83 (Ba  (None, 6, 6, 384)            1152      ['conv2d_83[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)          (None, 6, 6, 192)            245760    ['average_pooling2d_7[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_76 (Ba  (None, 6, 6, 320)            960       ['conv2d_76[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_78 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_78[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_79 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_79[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_82 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_82[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_83 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_83[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_84 (Ba  (None, 6, 6, 192)            576       ['conv2d_84[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_76 (Activation)  (None, 6, 6, 320)            0         ['batch_normalization_76[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed9_0 (Concatenate)      (None, 6, 6, 768)            0         ['activation_78[0][0]',       \n",
            "                                                                     'activation_79[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 6, 6, 768)            0         ['activation_82[0][0]',       \n",
            "                                                                     'activation_83[0][0]']       \n",
            "                                                                                                  \n",
            " activation_84 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_84[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed9 (Concatenate)        (None, 6, 6, 2048)           0         ['activation_76[0][0]',       \n",
            "                                                                     'mixed9_0[0][0]',            \n",
            "                                                                     'concatenate[0][0]',         \n",
            "                                                                     'activation_84[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_89 (Conv2D)          (None, 6, 6, 448)            917504    ['mixed9[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_89 (Ba  (None, 6, 6, 448)            1344      ['conv2d_89[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_89 (Activation)  (None, 6, 6, 448)            0         ['batch_normalization_89[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_86 (Conv2D)          (None, 6, 6, 384)            786432    ['mixed9[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_90 (Conv2D)          (None, 6, 6, 384)            1548288   ['activation_89[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_86 (Ba  (None, 6, 6, 384)            1152      ['conv2d_86[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_90 (Ba  (None, 6, 6, 384)            1152      ['conv2d_90[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_86 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_86[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_90 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_90[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_87 (Conv2D)          (None, 6, 6, 384)            442368    ['activation_86[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_88 (Conv2D)          (None, 6, 6, 384)            442368    ['activation_86[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_91 (Conv2D)          (None, 6, 6, 384)            442368    ['activation_90[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_92 (Conv2D)          (None, 6, 6, 384)            442368    ['activation_90[0][0]']       \n",
            "                                                                                                  \n",
            " average_pooling2d_8 (Avera  (None, 6, 6, 2048)           0         ['mixed9[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_85 (Conv2D)          (None, 6, 6, 320)            655360    ['mixed9[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_87 (Ba  (None, 6, 6, 384)            1152      ['conv2d_87[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_88 (Ba  (None, 6, 6, 384)            1152      ['conv2d_88[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_91 (Ba  (None, 6, 6, 384)            1152      ['conv2d_91[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_92 (Ba  (None, 6, 6, 384)            1152      ['conv2d_92[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2d_93 (Conv2D)          (None, 6, 6, 192)            393216    ['average_pooling2d_8[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_85 (Ba  (None, 6, 6, 320)            960       ['conv2d_85[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_87 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_87[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_88 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_88[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_91 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_91[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_92 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_92[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_93 (Ba  (None, 6, 6, 192)            576       ['conv2d_93[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_85 (Activation)  (None, 6, 6, 320)            0         ['batch_normalization_85[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed9_1 (Concatenate)      (None, 6, 6, 768)            0         ['activation_87[0][0]',       \n",
            "                                                                     'activation_88[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 6, 6, 768)            0         ['activation_91[0][0]',       \n",
            " )                                                                   'activation_92[0][0]']       \n",
            "                                                                                                  \n",
            " activation_93 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_93[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed10 (Concatenate)       (None, 6, 6, 2048)           0         ['activation_85[0][0]',       \n",
            "                                                                     'mixed9_1[0][0]',            \n",
            "                                                                     'concatenate_1[0][0]',       \n",
            "                                                                     'activation_93[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21802784 (83.17 MB)\n",
            "Trainable params: 21768352 (83.04 MB)\n",
            "Non-trainable params: 34432 (134.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# solution\n",
        "\n",
        "V3 = keras.applications.InceptionV3(include_top=False, weights='imagenet',input_shape=(256,256,3))\n",
        "V3.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq8urTnlx_ug"
      },
      "source": [
        "*    Add a global average pooling layer, followed by a fully-connected layer with 1024 neurons and then the classification layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "yTR4yCT-yPJk"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import GlobalAveragePooling2D, UpSampling2D\n",
        "\n",
        "model4=Sequential()\n",
        "model4.add(UpSampling2D((2,2)))              ## UpSampling increase the row and column of the data.Sometimes if we have less data so we can try to increase the data in this way.\n",
        "model4.add(UpSampling2D((2,2)))\n",
        "model4.add(UpSampling2D((2,2)))\n",
        "model4.add(V3)\n",
        "model4.add(GlobalAveragePooling2D())\n",
        "model4.add(Dense(1024, activation='relu'))\n",
        "model4.add(Dense(10, activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4.summary()"
      ],
      "metadata": {
        "id": "ICnDnbxyTZVj",
        "outputId": "148144be-1f70-477c-cc5d-17a72cda1c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-c9929e94922a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3480\u001b[0m         \"\"\"\n\u001b[1;32m   3481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3482\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   3483\u001b[0m                 \u001b[0;34m\"This model has not yet been built. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3484\u001b[0m                 \u001b[0;34m\"Build the model first by calling `build()` or by calling \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_rjXkO-yPfN"
      },
      "source": [
        "*   Train the model by freezing the base model. Train only the newly added layers.\n",
        "    *    Hint: Every layer has an attribute called 'trainable'\n",
        "*   Compile the model and train the model for a few epochs only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "3Szbku96ypDQ",
        "outputId": "136d2cbd-d0f4-4b31-81a1-123f6e80be3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-6fb0d9ea52d3>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3480\u001b[0m         \"\"\"\n\u001b[1;32m   3481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3482\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   3483\u001b[0m                 \u001b[0;34m\"This model has not yet been built. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3484\u001b[0m                 \u001b[0;34m\"Build the model first by calling `build()` or by calling \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
          ]
        }
      ],
      "source": [
        "# solution\n",
        "for layer in V3.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "model4.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4_data = model4.fit(x_train, y_train, batch_size=32, epochs=10, validation_split=0.1)"
      ],
      "metadata": {
        "id": "31MsK7TUIvGa",
        "outputId": "31063866-b459-4c96-ab4c-266e6cd57d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-f160fe6e95b7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel4_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 75, 75, 3), found shape=(None, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlQ5-83TypWy"
      },
      "source": [
        "*    Freeze the bottom layers and unfreeze the base layers.\n",
        "*    Compile and train the classifier with a very low learning rate (0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p-0Jyi6zDWG"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C8rryZozCr6"
      },
      "source": [
        "*    Compare the performance of the VGG model and the Inception-V3 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzhxMkt82ddr"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEA8k0BtzLyI"
      },
      "source": [
        "*    When do we train models from scratch? What are the potential issues in training models from scratch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ECBMya2e1g"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arkQv6N_66n2"
      },
      "source": [
        "*    Why do we use pre-trained weights?\n",
        "*    What is the difference between using random initialization and using weights from a pre-trained model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG2sGnxd64Sv"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-8E3dBw8Zoa"
      },
      "source": [
        "# Extracting features from Deep Networks\n",
        "\n",
        "It is quite possible to extract features (similar to SIFT or ORB) from different layers of deep network.\n",
        "\n",
        "*   Load ResNet50 model with imagenet weights and check the summary of the model\n",
        "*   Create a model to extract features from the 'avg_pool' layer.\n",
        "*   Extract features from the layer for all the train images.\n",
        "*   Use the extracted features to train a SVM classifier.\n",
        "    *    Use GridSearchCV and SVC to perform the classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ83bPO3mO16"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3CeHn372_J8"
      },
      "source": [
        "*    Evaluate the trained SVM model using the test set\n",
        "*    Calculate the accuracy score and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRwVH6bc3Ob6"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iNWSH-M1Lv1"
      },
      "source": [
        "# Feature Visualizations\n",
        "\n",
        "In order to visualize the features of a higher dimension data, t-SNE is used. t-SNE converts the affinities of the data points to probabilities. It recreates the probability distribution in a low-dimensional space. It is very helpful in visualizing features of different layers in a neural network.\n",
        "\n",
        "You can find more information about t-SNE [here](https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U84jVNvR39CD"
      },
      "source": [
        "*    Use TSNE to visualize the features extracted in the previous exercise.\n",
        "    *    Hint: TSNE function is available in the *sklearn.manifold* package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNTOB1gN4RC2"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOYsp8jZ4Rce"
      },
      "source": [
        "*    Why is feature visualization helpful?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5DlpPak4ZYG"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}