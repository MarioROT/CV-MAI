{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4iTyuU_Aidx"
   },
   "source": [
    "# GradCam: Visual Explanations via Gradient-based Localization and other visualization techniques.\n",
    "\n",
    "At the end of this lab, you will get familiarized with\n",
    "\n",
    "*    PCA, T-SNE and Umap visualization of layer results\n",
    "*   Applying GradCam for a CNN classification\n",
    "*   Visualizing gradcam results\n",
    "\n",
    "**Remember this is a graded exercise.**\n",
    "\n",
    "*   For every plot, make sure you provide appropriate titles, axis labels, legends, wherever applicable.\n",
    "*   Create reusable functions where ever possible, so that the code could be reused at different places.\n",
    "*   Add sufficient comments and explanations wherever necessary.\n",
    "*   **Once you have the code completed, use GPU to train model faster.**\n",
    "* This lab must be worked in pairs. Make sure that when you submit your lab you change the name to: **05_lab_Student1FullName_Student2FullName.ipynb**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPvzL66lA0vz"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f6rbINLAJqb"
   },
   "source": [
    "# Exercise 1: Features extraction (1.5 points)\n",
    "\n",
    "*In this section, we explored different techniques available to visualize via dimensionality reduction features developed through CNN models, first we train a model and extract its features. We use colorectal_histology from tfds leveraged before. Remember that this is multiclass problem with 8 possible categories.*\n",
    "\n",
    "\n",
    "1**.1 First, lets build a model and assess its performance:**\n",
    "*   Load \"colorectal_histology\" from TFDS\n",
    "*   Only keep a 20% of the dataset as test set, as we will not be training the model.\n",
    "*   Convert the labels to one-hot encoded form.\n",
    "*   Normalize the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyPRavgnJe6R"
   },
   "source": [
    "**1.2 Load a fine tune a MobileNet model pretrained on ImageNet**\n",
    "\n",
    "\n",
    "*   Load the model `05_lab_gradcam_model_exercise1_50epochs.keras` provided to you. This was trained for 50 epochs on this dataset, starting from the Imagenet pretrained MobileNet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9nOYdWe5vw0"
   },
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeTzzqZHETao"
   },
   "source": [
    "**1.3 Build a model to extract the features**\n",
    "\n",
    "*   Obtain the name of the layers. Look for *global_max_pooling2d* layer, as this will be the one where we extract the features from.\n",
    "*   Create a model that outputs the features from the chosen layer. Extract the intermediate features by using the predict function from the model with the test set. **Hint:** tensorflow.keras.models has a Model function that you can use to create the intermediate model. Explore its parameters\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLbfw_C1KdKi"
   },
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aorA5CgUD52L"
   },
   "source": [
    "## Exercise 2: Feature visualization with PCA (1 point)\n",
    "\n",
    "*Principal component analysis is a popular technique for analyzing large datasets containing a high number of dimensions by reducing dimensionality while preserving the maximum amount of information, and enabling the visualization of multidimensional data.*\n",
    "\n",
    "*For the features extracted, apply PCA and visualize the results. Comment on what you see and the advantages of this visualization in terms of explainability.*\n",
    "\n",
    "**2.1 Perform and visualize PCA:**\n",
    "\n",
    "*   Flatten the feature maps in order to use them in the PCA. You can do this with the reshape function.\n",
    "*   Perform PCA from sklearn.decomposition with 2 components. You can use sklearn function for this.\n",
    "*   Plot the resulting 2 components, color the observations by class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWhRRuKWAO0Y"
   },
   "outputs": [],
   "source": [
    "# solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U9TbFKFAPcw"
   },
   "source": [
    "## Exercise 3: Feature visualization with T-SNE (1 point)\n",
    "\n",
    "*In order to visualize the features of a higher dimension data, t-SNE can be used. t-SNE converts the affinities of the data points to probabilities. It recreates the probability distribution in a low-dimensional space. It is very helpful in visualizing features of different layers in a neural network.*\n",
    "\n",
    "*You can find more information about t-SNE [here](https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne)*\n",
    "\n",
    "**3.1 Use TSNE to visualize the features extracted from the neural network before. Obtain 2 components so you can easily visualize it in a 2-D plot, show your results**\n",
    "**Hint:** TSNE function is available in the *sklearn.manifold* package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jNkG7mtARxC"
   },
   "outputs": [],
   "source": [
    "# solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw4P1s53EJ5H"
   },
   "source": [
    "# Exercise 4: Visualize features with UMAP (1 point)\n",
    "\n",
    "*In this section, we will visualize different layers results via the dimensionality reduction technique UMAP (Uniform Manifold Approximation and Projection). This technique is a non-linear dimensionality reduction algorithm that preserves both local and global structures in the data. You can read more on its [paper](https://arxiv.org/abs/1802.03426)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS0P2evs46_s"
   },
   "source": [
    "**4.1 Use UMAP to visualize results of last layer for the test set. Follow this steps:**\n",
    "*   Apply the dimensionality reduction with umap from umap.umap_.\n",
    "*   Set the model to use 5 neighbors and a minimum distance of 0.1 with euclidean as distance metric.\n",
    "*   Once fitted and transform, visualize the UMAP embeddings as a scatterplot, with the classes as color. Use cmap = \"tab10\" inside the plt.scatter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkLoo_UV42JF"
   },
   "outputs": [],
   "source": [
    "# solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbrPuZA-E0m8"
   },
   "source": [
    "# Exercise 5: Applying GradCam with a pre-trained model (1.5 points)\n",
    "\n",
    " Gradient-weighted Class Activation Mapping (Grad-CAM) is a technique to visualize CNN's class decisions. It shows a visual explanation to make a more transparent models. It allows to see where the layer is focusing on to pick a class. You can learn more about GradCam from a code perspective on [keras tutorial](https://keras.io/examples/vision/grad_cam/) and theoretical aspects from the [paper](https://arxiv.org/abs/1610.02391)\n",
    "\n",
    "**5.1 Using a VGG16 already trained, you will apply gradcam to 5 ImageNet images provided in the folder \"data_imagenet_sample\".**\n",
    "\n",
    "*   Load the VGG16. model pretrained in ImageNet. Make sure to set include_top = True in this case as we are going to use ImageNet images.\n",
    "*   Look for the name of the last convolution layer. Save the name in order to plot its gradcam visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHQlODgwPvB1"
   },
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcTQ6c_VFWk1"
   },
   "outputs": [],
   "source": [
    "# solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAkw_KZxMDby"
   },
   "source": [
    "**5.2 Explore the 5 imagenet images:**\n",
    "\n",
    "*   Load the images provided in `data_imagenet_sample`\n",
    "*   Preprocessed them, according to what the model requires. Hint: look for the preprocess function in keras.applications.vgg16 (starter code provided)\n",
    "*   Predict the class for each image. Obtain the probability and label. For this, keras.applications.imagenet_utils provides a decode predictions function. A starter code for the function to do this is provided.\n",
    "*   Plot the image and print both the probability and label\n",
    "\n",
    "\n",
    "You can use this base code to load the images:\n",
    "```python\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "# Load and preprocess the ImageNet images\n",
    "def load_images(folder_path, images_names):\n",
    "  image_size = (224, 224)\n",
    "  images = []\n",
    "  # Load images\n",
    "  for image_name in images_names:\n",
    "    # Load\n",
    "    img = image.load_img(folder_path + image_name, target_size=image_size)\n",
    "    # Preprocess image\n",
    "    preprocessed_img = preprocess_input(image.img_to_array(img))\n",
    "    # Add to images\n",
    "    images.append(preprocessed_img)\n",
    "  return images\n",
    "\n",
    "# Prediction function\n",
    "def pred_model(input_batch, model):\n",
    "  # Remove last layer's softmax\n",
    "  model.layers[-1].activation = None\n",
    "  # Get predictions for input_batch: your code should go below\n",
    "  # Get decoded predictions: your code should go below\n",
    "  return decoded_predictions\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9ec56NmMB6b"
   },
   "outputs": [],
   "source": [
    "# solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meY0QLMFWhnL"
   },
   "source": [
    "**5.3 Create the gradcam heatmap:**\n",
    "*   The first step is to create a new model that maps the input image to the activations of the last convolutional layer and the output predictions.\n",
    "*   Use a GradientTape to compute the gradient of the top predicted class (or a chosen class) with respect to the activations of the last convolutional layer. Use the model created as first step for this.\n",
    "*   Calculate the relevance scores by taking the gradient of the output neuron (top predicted or chosen) with respect to the output feature map of the last convolutional layer.\n",
    "*   Calculate a vector where each entry is the mean intensity of the gradient over a specific feature map channel. Hint: use tf.reduce_mean\n",
    "*   Multiply each channel in the feature map array by its corresponding pooled gradient value to obtain the heatmap class activation.\n",
    "*   For visualization purposes, normalize the heatmap values between 0 and 1. Hint: Use the functions tf.maximum and tf.math.reduce_max\n",
    "* Make sure the final heatmap is a NumPy array that you can display with plt.matshow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibsGJ6qudony"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "  # Your code here\n",
    "  return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoK2TAB1xTbe"
   },
   "source": [
    "**5.4 Display the heatmap for the 5 images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Llvi4YwLe41Q"
   },
   "outputs": [],
   "source": [
    "# solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ali9JPVfsdP1"
   },
   "source": [
    "**5.5 Display a superimposed visualization of the 5 images:**\n",
    "\n",
    "Create a function that does the following:\n",
    "*   Rescale the heatmap values to the range of 0-255 and use the \"jet\" colormap to colorize the heatmap. Use the cm.get_cmap function\n",
    "*   Convert the colorized heatmap to a PIL (Python Imaging Library) image, resize it to the dimensions of the original input image, and convert it back to a NumPy array.\n",
    "*   Blend the colorized heatmap with the original image using the specified alpha value. Alpha should be a value that can be determined to set the transparency of the heatmap. This looks something like this: jet_heatmap * alpha + img. Then apply keras.utils.array_to_img to the result. This will create the superimposed image to be plotted with plt.imshow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVcmUL8Msb5q"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "def display_gradcam(img, heatmap, alpha=0.4):\n",
    "  # Your code here\n",
    "  return superimposed_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRC117XTLeRI"
   },
   "source": [
    "# Exercise 6: Apply GradCam to with Chest X-ray images dataset (3 points)\n",
    "\n",
    "*You have used this dataset in the last lab, it contains 5,863 Chest X-Ray images classified into 2 categories (Pneumonia/Normal). The normal chest X-ray shows clear lungs without any areas of abnormal opacification in the image. Bacterial pneumonia typically exhibits a focal lobar consolidation, whereas viral pneumonia manifests with a more diffuse ‘‘interstitial’’ pattern in both lungs.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpB5VOFdjNNI"
   },
   "source": [
    "A code to load the data and print a couple of images is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8xiTM_84Udv"
   },
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "input_shape = (128, 128, 3)\n",
    "batch_size = 32\n",
    "n_classes = 2\n",
    "\n",
    "folder_path_ex6 = \"your/data/path\"\n",
    "\n",
    "# Define the paths to your train, test, and validation directories\n",
    "train_dir = folder_path_ex6 + '/train'\n",
    "test_dir = folder_path_ex6 + '/test'\n",
    "val_dir = folder_path_ex6 + '/val'\n",
    "\n",
    "# Define the parameters for data augmentation and normalization\n",
    "batch_size = 64\n",
    "image_size = (128, 128)\n",
    "\n",
    "# Create an instance of the ImageDataGenerator and set the preprocessing options\n",
    "datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Load and preprocess the training data\n",
    "train_ds = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Load and preprocess the test data\n",
    "test_ds = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Load and preprocess the validation data\n",
    "valid_ds = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Get the class indices from the flow_from_directory output\n",
    "class_indices = train_ds.class_indices\n",
    "\n",
    "# Class names\n",
    "class_names = {v: k for k, v in class_indices.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKzkeoxn7EtA"
   },
   "outputs": [],
   "source": [
    "# Printing 5 images\n",
    "n_samples = 5\n",
    "idx = {}\n",
    "\n",
    "# Getting index per each class\n",
    "for number in range(n_classes):\n",
    "    indices = np.where(train_ds.classes == number)[0][:n_samples]\n",
    "    idx[number] = indices\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(n_samples, n_classes, figsize=(10, 10))\n",
    "\n",
    "for class_i, indices in idx.items():\n",
    "    for id, idx in enumerate(indices):\n",
    "        batch_x, batch_y = train_ds[idx // batch_size]\n",
    "        image = batch_x[idx % batch_size]\n",
    "        axes[id, class_i].imshow(image, cmap=plt.get_cmap('gray'))\n",
    "        axes[id, class_i].set_title(f'Image {id+1}, class {class_i}', fontsize=5)\n",
    "        axes[id, class_i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q4dRGLSDr0n"
   },
   "source": [
    "**2.2 Apply GradCam to train models. For this consider:**\n",
    "*  We are providing you with 2 models `05_lab_gradcam_model_10epochs.keras` and `05_lab_gradcam_model_25epochs.keras`. One was trained for 10 epochs and the other one for 25 epochs with a ResNet50. Load them into your environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqEYWCPNAV-M"
   },
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k87v0IzFi60l"
   },
   "source": [
    "**2.3 Obtain your gradcam visualization for `both` models:**\n",
    "* Obtain the gradcam superimposed visualization for 5 images. Choose the last convolutional layer to do this.\n",
    "* Also print the actual and predicted class\n",
    "* Comment on the results - which one you think is making more sense?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLcwpObNelFz"
   },
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wS93L-Bfl0M_"
   },
   "source": [
    "Comment on the differences:\n",
    "\n",
    "**SOLUTION**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDzZRBYzoqZ9"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **End of Lab 04: Interpretation with GradCAM**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
