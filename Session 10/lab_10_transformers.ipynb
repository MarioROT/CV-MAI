{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ7Rk_R4HIJn"
      },
      "source": [
        "# Laboratory #10 : Vision Transformers\n",
        "\n",
        "At the end of this laboratory, you would get familiarized with\n",
        "\n",
        "*   Vision Transformers\n",
        "\n",
        "**Remember this is a graded exercise.**\n",
        "\n",
        "*   For every plot, make sure you provide appropriate titles, axis labels, legends, wherever applicable.\n",
        "*   Create reusable functions where ever possible, so that the code could be reused at different places.\n",
        "*   Add sufficient comments and explanations wherever necessary.\n",
        "*   **Once you have the code completed, use GPU to train model faster.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQRtFkg6FSlG"
      },
      "source": [
        "# Vision Transformers\n",
        "\n",
        "Vision Transformers were introduced by Alexey Dosovitskiy et al. in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "Read the ViT paper and answer the below descriptive questions.\n",
        "\n",
        "*   What are the differences between CNN and Transformers?\n",
        "*   What is Multi-head self-attention?\n",
        "*   What is Inductive Bias?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lyt2KtJiEM_"
      },
      "source": [
        "**Solution**\n",
        "- What are the differences between CNN and Transformers?\n",
        "  - CNNs (Convolutional Neural Networks) are commonly used in computer vision tasks and have been the standard approach for image recognition. They rely on convolutional layers to extract local features from images and use pooling layers to downsample the feature maps.Transformers, on the other hand, have been widely used in natural language processing tasks but have limited applications in computer vision. They are based on self-attention mechanisms that allow them to capture global dependencies in the input sequence.\n",
        "\n",
        "  - Unlike CNNs, which operate on local patches of an image, Transformers can directly process sequences of image patches without the need for convolutional layers. This makes them more flexible and efficient in terms of computational resources required for training.\n",
        "  - CNNs have inherent inductive biases such as translation equivariance and locality, which make them effective in tasks with limited data. Transformers lack these biases and may not generalize well when trained on insufficient amounts of data.\n",
        "  - While CNNs are well-suited for tasks with spatial information, Transformers are better at capturing long-range dependencies and global context, making them suitable for tasks that require understanding relationships between different parts of an image.\n",
        "\n",
        "- What is Multi-head self-attention?\n",
        "  - Multi-head self-attention is a mechanism used in Transformers that allows the model to attend to different parts of the input sequence simultaneously. It involves splitting the input into multiple smaller representations, called heads, and applying self-attention independently on each head.\n",
        "\n",
        "  - Each head learns different relationships and captures different aspects of the input sequence, enabling the model to capture both local and global dependencies effectively.\n",
        "\n",
        "  - The outputs of the multiple attention heads are then concatenated and linearly transformed to obtain the final representation. This allows the model to capture different types of information and improve its ability to understand complex patterns in the input.\n",
        "\n",
        "- Inductive Bias\n",
        "  - Inductive bias are assumptions or biases that a ML model includes during the learning process to make predictions based on limited training data.\n",
        "\n",
        "  - CNNs have inherent inductive biases such as locality, two-dimensional neighborhood structure, and translation equivariance. These biases are baked into each layer of the CNN model, allowing it to effectively capture spatial information and patterns in images.\n",
        "\n",
        "  - In contrast, Vision Transformers (ViTs) have much less image-specific inductive bias than CNNs. While ViTs use self-attention layers to capture global dependencies in the input sequence, they do not have the same level of built-in assumptions about spatial relationships. ViTs rely on the learning process to discover and understand spatial relations between image patches.\n",
        "\n",
        "  - The reduced inductive bias in ViTs allows them to be more flexible and adaptable to different image recognition tasks, but it also means that they may require larger amounts of training data to generalize effectively.\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blK0wdXkGxWM"
      },
      "source": [
        "*   Explain the model architecture of ViT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2kTURmHHVUB"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "The Vision Transformer (ViT) model architecture is based on the regulat Transformer architecture.\n",
        " 1. ViT takes an image as input and divides it into a sequence of fixed-size non-overlapping patches.\n",
        " 2. Each patch is linearly projected to a lower-dimensional representation, called the patch embedding, which serves as the input to the Transformer encoder.\n",
        " 3. The Transformer encoder consists of multiple layers, each containing a multi-head self-attention mechanism and a feed-forward neural network. Layers included follow  Normalization --> Multi-Head Attention --> Normalization --> MLP.\n",
        " 4. The self-attention mechanism allows the model to capture global dependencies between the patches, while the feed-forward network applies non-linear transformations to the patch embeddings.\n",
        " 5. The outputs of the Transformer encoder are then passed through a classification head, which maps the sequence of patch embeddings to the final output classes.\n",
        "\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3fPvtIgUo_R"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "We will implement the Vision Transformer (ViT) model for image classification and demonstrates it on the CIFAR-100 dataset.\n",
        "The ViT model applies the Transformer architecture with self-attention to sequences of image patches, without using convolution layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfv-aT740MVJ"
      },
      "source": [
        "## Setup\n",
        "For this exercise, we require TensorFlow 2.4 or higher, as well as\n",
        "[TensorFlow Addons](https://www.tensorflow.org/addons/overview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0qx2czPY7sIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1315c1-0266-4a84-a58d-f0e9546f6221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "pip install -U tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vLf_7xWI8NB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45dc4d5-a26f-4180-d6d3-9a45503c9718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKnbipxm0fn6"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "We will use the CIFAR-100 dataset available in tf.keras.datasets package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aUv0hh9o8P58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa7a5e42-3490-413e-9622-458f57fb118e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 11s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2op-WxA0l9t"
      },
      "source": [
        "## Configure the Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5kDxBNE08TCQ"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2YBwB800wvu"
      },
      "source": [
        "## Data Augmentation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TrPHMK6y8Uzk"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFoYrR3o04qr"
      },
      "source": [
        "## Implement a Multi-layer Perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mRwIT9Ru8W8Y"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDIDnGh_0_C8"
      },
      "source": [
        "## Implementing Patch Creation Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Cr7K3R1T8XTd"
      },
      "outputs": [],
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94eYhR3HIlu1"
      },
      "source": [
        "### Visualizing the patches for a sample image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vZ_AO9m58b5n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "216203b4-7dfd-434b-a66d-47cb13123546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 72 X 72\n",
            "Patch size: 6 X 6\n",
            "Patches per image: 144\n",
            "Elements per patch: 108\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 144 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS-pwhwwIzIU"
      },
      "source": [
        "## Patch Encoding Layer\n",
        "\n",
        "*   What is the role of Patch Encoding Layer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJux_IyZI_2G"
      },
      "source": [
        "**Solution**\n",
        "- Each patch is linearly projected to a lower-dimensional embeddin, the patch embeddings capture local information from the image patches and serve as the input to the Transformer encoder.The Transformer encoder then utilizes self-attention mechanisms to capture global dependencies between the patch embeddings, allowing the model to understand the relationships between different parts of the image.\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GxcuieU78eFg"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnQ-V_qbJDI5"
      },
      "source": [
        "*   What happens in the call function of the PatchEncoder class?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJDp_fmmJDiO"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "1. Projects the input patch using the dense layer.\n",
        "2. Retrieves positional embeddings using the embedding layer based on the positions of the patches.\n",
        "3. Adds the projected patch and positional embeddings element-wise.\n",
        "4. Returns the encoded result\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsYeLjTWJMUS"
      },
      "source": [
        "## Building the ViT Model\n",
        "\n",
        "*   We will use the MultiHeadAttention layer as self-attention for this implementation\n",
        "*   PS: There are many changes to the original paper for ease of execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IaLpri_v8gse"
      },
      "outputs": [],
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWD8Wcq2Jr6I"
      },
      "source": [
        "*   Explain the execution of the ViT model. What is the sequence of operations?\n",
        "*   What is skip connection? Why is it needed?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HxC1M3YJ6m2"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "* **Explain the execution of the ViT model. What is the sequence of operations?**\n",
        "  - Input Processing:\n",
        "    - The input image is received through the inputs layer.\n",
        "    - Data augmentation is applied.\n",
        "  - Patching:\n",
        "    - The augmented image is divided into non-overlapping patches.\n",
        "  - Patch Encoding:\n",
        "    - Each patch is encoded using the PatchEncoder layer, by projecting it in a low-dimension representation and an embedding layer for positional information.\n",
        "  - Transformer Blocks:\n",
        "    - For each specified transformer block:\n",
        "      - Layer normalization is applied to the encoded patches.\n",
        "      - Multi-head self-attention processing is performed on the normalized patches.\n",
        "      - The result of the attention operation is added to the original encoded patches using a skip connection.\n",
        "      - Another layer normalization is applied to the output.\n",
        "      - A MLP is applied to the output of the attention operation.\n",
        "      - The output of the MLP is added to the output of the attention operation using another skip connection.\n",
        "  - Global Feature Representation:\n",
        "    - Layer normalization is applied to the final encoded patches.\n",
        "    - The representation is flattened, and a dropout layer is added for regularization.\n",
        "  - Final Classification Head:\n",
        "    - An MLP is applied to the flattened representation.\n",
        "    - The output of the MLP is used for classification through a dense layer.\n",
        "\n",
        "- **What is skip connection? Why is it needed?**\n",
        "\n",
        "  - It is a shortcut connection that bypasses one or more layers. The purpose of skip connections is to facilitate the flow of gradients during training, which helps in mitigating the vanishing gradient problem. They also allow the model to learn identity mappings, making it easier to learn more complex representations. The skip connections essentially provide a direct path for information to flow through the network, helping in the training of deeper models.\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvkeA4xgKsnz"
      },
      "source": [
        "*   What is self-attention?\n",
        "*   What is the problem in using global self-attention?\n",
        "*   What can be used instead of global self-attention?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGiS4ceeK77i"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "- **What is self-attention?**\n",
        "  - It is a mechanism that allows a set of items in a sequence to focus on different parts of itself when processing information.\n",
        "\n",
        "  - In NLP, self-attention is particularly useful for capturing relationships and dependencies between different words in a sentence. It enables the model to weigh the importance of each word in the context of the entire sequence, allowing for flexible and non-local information integration.\n",
        "\n",
        "  - The self-attention mechanism operates on a set of input vectors (a.k.a. embeddings) and computes a set of output vectors, where each output vector is a weighted sum of the input vectors. The weights are determined dynamically based on the similarity or relevance between the elements in the input sequence. The attention scores are calculated using a compatibility function followed by a softmax operation to obtain normalized weights.\n",
        "- **What is the problem in using global self-attention?**\n",
        "  -\n",
        "Global self-attention has computational challenges due to its quadratic time and space complexity with respect to the sequence length. The dot-product computation for attention scores results in $O(n^2)$ time complexity, making it computationally expensive for long sequences, and the quadratic space complexity of the attention matrix is memory-intensive. This limitation hinders the scalability of global self-attention for large datasets and real-world applications.\n",
        "\n",
        "- **What can be used instead of global self-attention?**\n",
        "  - To address global self-attention, variants like local attention, sparse attention, and approximate attention have been introduced, offering more efficient alternatives that maintain the benefits of self-attention while making it practical for processing long sequences.\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3_1FlgsJ9hB"
      },
      "source": [
        "## Training the ViT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WDVSlD838jkH"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJKA65VEKE_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80e0057-ac51-4aa0-d51c-29bacf3e2aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "176/176 [==============================] - 79s 368ms/step - loss: 4.5140 - accuracy: 0.0406 - top-5-accuracy: 0.1450 - val_loss: 3.9842 - val_accuracy: 0.0926 - val_top-5-accuracy: 0.2782\n",
            "Epoch 2/100\n",
            "176/176 [==============================] - 65s 370ms/step - loss: 3.9911 - accuracy: 0.0886 - top-5-accuracy: 0.2753 - val_loss: 3.6037 - val_accuracy: 0.1452 - val_top-5-accuracy: 0.3966\n",
            "Epoch 3/100\n",
            "176/176 [==============================] - 70s 399ms/step - loss: 3.7326 - accuracy: 0.1248 - top-5-accuracy: 0.3514 - val_loss: 3.3689 - val_accuracy: 0.1896 - val_top-5-accuracy: 0.4620\n",
            "Epoch 4/100\n",
            "176/176 [==============================] - 67s 381ms/step - loss: 3.5442 - accuracy: 0.1542 - top-5-accuracy: 0.4117 - val_loss: 3.2234 - val_accuracy: 0.2236 - val_top-5-accuracy: 0.4982\n",
            "Epoch 5/100\n",
            "176/176 [==============================] - 66s 378ms/step - loss: 3.3889 - accuracy: 0.1848 - top-5-accuracy: 0.4530 - val_loss: 3.0503 - val_accuracy: 0.2504 - val_top-5-accuracy: 0.5412\n",
            "Epoch 6/100\n",
            "176/176 [==============================] - 66s 376ms/step - loss: 3.2507 - accuracy: 0.2082 - top-5-accuracy: 0.4904 - val_loss: 2.9289 - val_accuracy: 0.2710 - val_top-5-accuracy: 0.5708\n",
            "Epoch 7/100\n",
            "176/176 [==============================] - 66s 378ms/step - loss: 3.1314 - accuracy: 0.2324 - top-5-accuracy: 0.5230 - val_loss: 2.8150 - val_accuracy: 0.2914 - val_top-5-accuracy: 0.6010\n",
            "Epoch 8/100\n",
            "176/176 [==============================] - 67s 380ms/step - loss: 3.0180 - accuracy: 0.2535 - top-5-accuracy: 0.5520 - val_loss: 2.7183 - val_accuracy: 0.3112 - val_top-5-accuracy: 0.6238\n",
            "Epoch 9/100\n",
            "176/176 [==============================] - 68s 385ms/step - loss: 2.9123 - accuracy: 0.2734 - top-5-accuracy: 0.5770 - val_loss: 2.6494 - val_accuracy: 0.3312 - val_top-5-accuracy: 0.6428\n",
            "Epoch 10/100\n",
            "176/176 [==============================] - 71s 406ms/step - loss: 2.8107 - accuracy: 0.2950 - top-5-accuracy: 0.6020 - val_loss: 2.5516 - val_accuracy: 0.3476 - val_top-5-accuracy: 0.6596\n",
            "Epoch 11/100\n",
            "176/176 [==============================] - 67s 380ms/step - loss: 2.7207 - accuracy: 0.3114 - top-5-accuracy: 0.6221 - val_loss: 2.5554 - val_accuracy: 0.3572 - val_top-5-accuracy: 0.6574\n",
            "Epoch 12/100\n",
            "176/176 [==============================] - 66s 374ms/step - loss: 2.6474 - accuracy: 0.3269 - top-5-accuracy: 0.6396 - val_loss: 2.4453 - val_accuracy: 0.3728 - val_top-5-accuracy: 0.6772\n",
            "Epoch 13/100\n",
            "176/176 [==============================] - 71s 404ms/step - loss: 2.5580 - accuracy: 0.3433 - top-5-accuracy: 0.6588 - val_loss: 2.3946 - val_accuracy: 0.3820 - val_top-5-accuracy: 0.6898\n",
            "Epoch 14/100\n",
            "176/176 [==============================] - 68s 389ms/step - loss: 2.4944 - accuracy: 0.3595 - top-5-accuracy: 0.6722 - val_loss: 2.3332 - val_accuracy: 0.3980 - val_top-5-accuracy: 0.7012\n",
            "Epoch 15/100\n",
            "176/176 [==============================] - 66s 373ms/step - loss: 2.4283 - accuracy: 0.3697 - top-5-accuracy: 0.6847 - val_loss: 2.2943 - val_accuracy: 0.4026 - val_top-5-accuracy: 0.7076\n",
            "Epoch 16/100\n",
            "176/176 [==============================] - 70s 398ms/step - loss: 2.3617 - accuracy: 0.3852 - top-5-accuracy: 0.7016 - val_loss: 2.2929 - val_accuracy: 0.4060 - val_top-5-accuracy: 0.7106\n",
            "Epoch 17/100\n",
            "176/176 [==============================] - 71s 404ms/step - loss: 2.3156 - accuracy: 0.3945 - top-5-accuracy: 0.7132 - val_loss: 2.2310 - val_accuracy: 0.4198 - val_top-5-accuracy: 0.7206\n",
            "Epoch 18/100\n",
            "176/176 [==============================] - 70s 399ms/step - loss: 2.2521 - accuracy: 0.4088 - top-5-accuracy: 0.7249 - val_loss: 2.1897 - val_accuracy: 0.4354 - val_top-5-accuracy: 0.7226\n",
            "Epoch 19/100\n",
            "176/176 [==============================] - 66s 375ms/step - loss: 2.1942 - accuracy: 0.4239 - top-5-accuracy: 0.7364 - val_loss: 2.1523 - val_accuracy: 0.4432 - val_top-5-accuracy: 0.7352\n",
            "Epoch 20/100\n",
            "176/176 [==============================] - 66s 374ms/step - loss: 2.1549 - accuracy: 0.4308 - top-5-accuracy: 0.7466 - val_loss: 2.1187 - val_accuracy: 0.4468 - val_top-5-accuracy: 0.7408\n",
            "Epoch 21/100\n",
            "176/176 [==============================] - 66s 378ms/step - loss: 2.0925 - accuracy: 0.4432 - top-5-accuracy: 0.7559 - val_loss: 2.1069 - val_accuracy: 0.4496 - val_top-5-accuracy: 0.7446\n",
            "Epoch 22/100\n",
            "176/176 [==============================] - 66s 375ms/step - loss: 2.0529 - accuracy: 0.4522 - top-5-accuracy: 0.7638 - val_loss: 2.0517 - val_accuracy: 0.4572 - val_top-5-accuracy: 0.7566\n",
            "Epoch 23/100\n",
            "176/176 [==============================] - 71s 403ms/step - loss: 1.9994 - accuracy: 0.4660 - top-5-accuracy: 0.7758 - val_loss: 2.0443 - val_accuracy: 0.4664 - val_top-5-accuracy: 0.7554\n",
            "Epoch 24/100\n",
            "176/176 [==============================] - 65s 368ms/step - loss: 1.9617 - accuracy: 0.4706 - top-5-accuracy: 0.7813 - val_loss: 2.0484 - val_accuracy: 0.4626 - val_top-5-accuracy: 0.7618\n",
            "Epoch 25/100\n",
            "176/176 [==============================] - 71s 403ms/step - loss: 1.9060 - accuracy: 0.4829 - top-5-accuracy: 0.7940 - val_loss: 1.9979 - val_accuracy: 0.4706 - val_top-5-accuracy: 0.7694\n",
            "Epoch 26/100\n",
            "176/176 [==============================] - 67s 379ms/step - loss: 1.8611 - accuracy: 0.4951 - top-5-accuracy: 0.7996 - val_loss: 1.9921 - val_accuracy: 0.4742 - val_top-5-accuracy: 0.7706\n",
            "Epoch 27/100\n",
            "176/176 [==============================] - 66s 375ms/step - loss: 1.8191 - accuracy: 0.5027 - top-5-accuracy: 0.8088 - val_loss: 1.9829 - val_accuracy: 0.4828 - val_top-5-accuracy: 0.7732\n",
            "Epoch 28/100\n",
            "176/176 [==============================] - 66s 376ms/step - loss: 1.7785 - accuracy: 0.5154 - top-5-accuracy: 0.8163 - val_loss: 1.9513 - val_accuracy: 0.4874 - val_top-5-accuracy: 0.7796\n",
            "Epoch 29/100\n",
            "176/176 [==============================] - 66s 374ms/step - loss: 1.7512 - accuracy: 0.5181 - top-5-accuracy: 0.8204 - val_loss: 1.9428 - val_accuracy: 0.4904 - val_top-5-accuracy: 0.7752\n",
            "Epoch 30/100\n",
            "176/176 [==============================] - 64s 366ms/step - loss: 1.7083 - accuracy: 0.5269 - top-5-accuracy: 0.8292 - val_loss: 1.9463 - val_accuracy: 0.4898 - val_top-5-accuracy: 0.7786\n",
            "Epoch 31/100\n",
            "176/176 [==============================] - 70s 398ms/step - loss: 1.6696 - accuracy: 0.5378 - top-5-accuracy: 0.8362 - val_loss: 1.9027 - val_accuracy: 0.4992 - val_top-5-accuracy: 0.7838\n",
            "Epoch 32/100\n",
            "176/176 [==============================] - 71s 404ms/step - loss: 1.6429 - accuracy: 0.5443 - top-5-accuracy: 0.8411 - val_loss: 1.9036 - val_accuracy: 0.5102 - val_top-5-accuracy: 0.7880\n",
            "Epoch 33/100\n",
            "176/176 [==============================] - 71s 404ms/step - loss: 1.6064 - accuracy: 0.5517 - top-5-accuracy: 0.8458 - val_loss: 1.8753 - val_accuracy: 0.5112 - val_top-5-accuracy: 0.7914\n",
            "Epoch 34/100\n",
            "176/176 [==============================] - 65s 369ms/step - loss: 1.5681 - accuracy: 0.5604 - top-5-accuracy: 0.8533 - val_loss: 1.8898 - val_accuracy: 0.5064 - val_top-5-accuracy: 0.7892\n",
            "Epoch 35/100\n",
            "176/176 [==============================] - 65s 368ms/step - loss: 1.5389 - accuracy: 0.5676 - top-5-accuracy: 0.8581 - val_loss: 1.8969 - val_accuracy: 0.5074 - val_top-5-accuracy: 0.7890\n",
            "Epoch 36/100\n",
            "176/176 [==============================] - 71s 406ms/step - loss: 1.5175 - accuracy: 0.5718 - top-5-accuracy: 0.8606 - val_loss: 1.8598 - val_accuracy: 0.5136 - val_top-5-accuracy: 0.7962\n",
            "Epoch 37/100\n",
            "176/176 [==============================] - 66s 376ms/step - loss: 1.4881 - accuracy: 0.5800 - top-5-accuracy: 0.8666 - val_loss: 1.8663 - val_accuracy: 0.5164 - val_top-5-accuracy: 0.7956\n",
            "Epoch 38/100\n",
            "176/176 [==============================] - 71s 405ms/step - loss: 1.4575 - accuracy: 0.5874 - top-5-accuracy: 0.8694 - val_loss: 1.8696 - val_accuracy: 0.5174 - val_top-5-accuracy: 0.7938\n",
            "Epoch 39/100\n",
            "176/176 [==============================] - 65s 368ms/step - loss: 1.4384 - accuracy: 0.5902 - top-5-accuracy: 0.8760 - val_loss: 1.8982 - val_accuracy: 0.5056 - val_top-5-accuracy: 0.7964\n",
            "Epoch 40/100\n",
            "176/176 [==============================] - 64s 366ms/step - loss: 1.4099 - accuracy: 0.6006 - top-5-accuracy: 0.8770 - val_loss: 1.8785 - val_accuracy: 0.5154 - val_top-5-accuracy: 0.7944\n",
            "Epoch 41/100\n",
            "176/176 [==============================] - 71s 405ms/step - loss: 1.3976 - accuracy: 0.6026 - top-5-accuracy: 0.8819 - val_loss: 1.8749 - val_accuracy: 0.5216 - val_top-5-accuracy: 0.8028\n",
            "Epoch 42/100\n",
            "176/176 [==============================] - 65s 368ms/step - loss: 1.3581 - accuracy: 0.6094 - top-5-accuracy: 0.8862 - val_loss: 1.8716 - val_accuracy: 0.5212 - val_top-5-accuracy: 0.8014\n",
            "Epoch 43/100\n",
            "176/176 [==============================] - 64s 366ms/step - loss: 1.3456 - accuracy: 0.6178 - top-5-accuracy: 0.8886 - val_loss: 1.8669 - val_accuracy: 0.5136 - val_top-5-accuracy: 0.8002\n",
            "Epoch 44/100\n",
            "176/176 [==============================] - 71s 404ms/step - loss: 1.3268 - accuracy: 0.6191 - top-5-accuracy: 0.8896 - val_loss: 1.8581 - val_accuracy: 0.5218 - val_top-5-accuracy: 0.8022\n",
            "Epoch 45/100\n",
            "176/176 [==============================] - 65s 368ms/step - loss: 1.2949 - accuracy: 0.6285 - top-5-accuracy: 0.8956 - val_loss: 1.8699 - val_accuracy: 0.5218 - val_top-5-accuracy: 0.8026\n",
            "Epoch 46/100\n",
            "176/176 [==============================] - 71s 403ms/step - loss: 1.2838 - accuracy: 0.6313 - top-5-accuracy: 0.8981 - val_loss: 1.8426 - val_accuracy: 0.5284 - val_top-5-accuracy: 0.8042\n",
            "Epoch 47/100\n",
            "176/176 [==============================] - 65s 367ms/step - loss: 1.2650 - accuracy: 0.6374 - top-5-accuracy: 0.9013 - val_loss: 1.8584 - val_accuracy: 0.5228 - val_top-5-accuracy: 0.8044\n",
            "Epoch 48/100\n",
            "176/176 [==============================] - 64s 366ms/step - loss: 1.2461 - accuracy: 0.6399 - top-5-accuracy: 0.9032 - val_loss: 1.8317 - val_accuracy: 0.5272 - val_top-5-accuracy: 0.8080\n",
            "Epoch 49/100\n",
            "176/176 [==============================] - 66s 373ms/step - loss: 1.2269 - accuracy: 0.6447 - top-5-accuracy: 0.9050 - val_loss: 1.8622 - val_accuracy: 0.5304 - val_top-5-accuracy: 0.8080\n",
            "Epoch 50/100\n",
            "176/176 [==============================] - 65s 367ms/step - loss: 1.2179 - accuracy: 0.6460 - top-5-accuracy: 0.9082 - val_loss: 1.8280 - val_accuracy: 0.5262 - val_top-5-accuracy: 0.8080\n",
            "Epoch 51/100\n",
            "176/176 [==============================] - 65s 367ms/step - loss: 1.1952 - accuracy: 0.6531 - top-5-accuracy: 0.9122 - val_loss: 1.8723 - val_accuracy: 0.5252 - val_top-5-accuracy: 0.8032\n",
            "Epoch 52/100\n",
            "176/176 [==============================] - 71s 406ms/step - loss: 1.1733 - accuracy: 0.6585 - top-5-accuracy: 0.9146 - val_loss: 1.8555 - val_accuracy: 0.5310 - val_top-5-accuracy: 0.8090\n",
            "Epoch 53/100\n",
            "176/176 [==============================] - 65s 368ms/step - loss: 1.1783 - accuracy: 0.6575 - top-5-accuracy: 0.9131 - val_loss: 1.8639 - val_accuracy: 0.5284 - val_top-5-accuracy: 0.8098\n",
            "Epoch 54/100\n",
            " 52/176 [=======>......................] - ETA: 44s - loss: 1.1003 - accuracy: 0.6810 - top-5-accuracy: 0.9240"
          ]
        }
      ],
      "source": [
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F-UBiR_KGVr"
      },
      "source": [
        "### Creating history plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUtcXmmYBBun"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNpi8W_xKYnC"
      },
      "source": [
        "### Evaluating the trained ViT transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y2v4BhxBFHs"
      },
      "outputs": [],
      "source": [
        "loss, accuracy, top_5_accuracy = vit_classifier.evaluate(x_test, y_test)\n",
        "print(f\"Test loss: {round(loss, 2)}\")\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDy8XAvEm830"
      },
      "source": [
        "# Shifted Patch Tokenization\n",
        "\n",
        "In a ViT pipeline, the input images are divided into patches that are then linearly projected into tokens. Shifted patch tokenization (STP) is introduced to combat the low receptive field of ViTs. The steps for Shifted Patch Tokenization are as follows:\n",
        "*   Start with an image.\n",
        "*   Shift the image in diagonal directions.\n",
        "*   Concat the diagonally shifted images with the original image.\n",
        "*   Extract patches of the concatenated images.\n",
        "*   Flatten the spatial dimension of all patches.\n",
        "*   Layer normalize the flattened patches and then project it.\n",
        "\n",
        "More details about the implementation can be read from the paper [Vision Transformer for Small-Size Datasets](https://arxiv.org/abs/2112.13492v1)\n",
        "\n",
        "---\n",
        "*   Implement a shiftedpatchtokenizer class\n",
        "*   The class should implement the __init__() and call() functions\n",
        "*   Implement left-up, right-up, left-down and right-down shift operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0upIqeZ1RL5"
      },
      "outputs": [],
      "source": [
        "class ShiftedPatchTokenization(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_patches=NUM_PATCHES,\n",
        "        projection_dim=PROJECTION_DIM,\n",
        "        vanilla=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vanilla = vanilla  # Flag to swtich to vanilla patch extractor\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.half_patch = patch_size // 2\n",
        "        self.flatten_patches = layers.Reshape((num_patches, -1))\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
        "\n",
        "    def crop_shift_pad(self, images, mode):\n",
        "        # Build the diagonally shifted images\n",
        "        if mode == \"left-up\":\n",
        "            crop_height = self.half_patch\n",
        "            crop_width = self.half_patch\n",
        "            shift_height = 0\n",
        "            shift_width = 0\n",
        "        elif mode == \"left-down\":\n",
        "            crop_height = 0\n",
        "            crop_width = self.half_patch\n",
        "            shift_height = self.half_patch\n",
        "            shift_width = 0\n",
        "        elif mode == \"right-up\":\n",
        "            crop_height = self.half_patch\n",
        "            crop_width = 0\n",
        "            shift_height = 0\n",
        "            shift_width = self.half_patch\n",
        "        else:\n",
        "            crop_height = 0\n",
        "            crop_width = 0\n",
        "            shift_height = self.half_patch\n",
        "            shift_width = self.half_patch\n",
        "\n",
        "        # Crop the shifted images and pad them\n",
        "        crop = tf.image.crop_to_bounding_box(\n",
        "            images,\n",
        "            offset_height=crop_height,\n",
        "            offset_width=crop_width,\n",
        "            target_height=self.image_size - self.half_patch,\n",
        "            target_width=self.image_size - self.half_patch,\n",
        "        )\n",
        "        shift_pad = tf.image.pad_to_bounding_box(\n",
        "            crop,\n",
        "            offset_height=shift_height,\n",
        "            offset_width=shift_width,\n",
        "            target_height=self.image_size,\n",
        "            target_width=self.image_size,\n",
        "        )\n",
        "        return shift_pad\n",
        "\n",
        "    def call(self, images):\n",
        "        if not self.vanilla:\n",
        "            # Concat the shifted images with the original image\n",
        "            images = tf.concat(\n",
        "                [\n",
        "                    images,\n",
        "                    self.crop_shift_pad(images, mode=\"left-up\"),\n",
        "                    self.crop_shift_pad(images, mode=\"left-down\"),\n",
        "                    self.crop_shift_pad(images, mode=\"right-up\"),\n",
        "                    self.crop_shift_pad(images, mode=\"right-down\"),\n",
        "                ],\n",
        "                axis=-1,\n",
        "            )\n",
        "        # Patchify the images and flatten it\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        flat_patches = self.flatten_patches(patches)\n",
        "        if not self.vanilla:\n",
        "            # Layer normalize the flat patches and linearly project it\n",
        "            tokens = self.layer_norm(flat_patches)\n",
        "            tokens = self.projection(tokens)\n",
        "        else:\n",
        "            # Linearly project the flat patches\n",
        "            tokens = self.projection(flat_patches)\n",
        "        return (tokens, patches)\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(\n",
        "        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "\n",
        "    def call(self, encoded_patches):\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_patches = encoded_patches + encoded_positions\n",
        "        return encoded_patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBvcw4BKL6Fi"
      },
      "source": [
        "*   Visualize the patches for a random image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVC3fX4AL8LN"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z1NjMKI1Rme"
      },
      "source": [
        "## Build the ViT model\n",
        "\n",
        "*   Create the ViT classifier by replacing the Patch Layer with the Shifted Patch Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg0fDXqj1bpn"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "def create_vit_classifier_shifted_path_layer():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    (tokens, _) = ShiftedPatchTokenization(vanilla=False)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder()(tokens)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mla5r-oE1kBm"
      },
      "source": [
        "*   Compile and train the classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH3VD6QpMzjM"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "vit_classifier_SPL = create_vit_classifier_shifted_path_layer()\n",
        "history_SPL = run_experiment(vit_classifier_SPL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM8HOAZIM2yn"
      },
      "source": [
        "*   Create the history plot for the new ViT model\n",
        "*   Evaluate the model using the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSIZ8gF9NEeM"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "plt.plot(history_SPL.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(historySPL.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "loss_SPL, accuracy_SPL, top_5_accuracy_SPL = vit_classifier_SPL.evaluate(x_test, y_test)\n",
        "print(f\"Test loss: {round(loss_SPL, 2)}\")\n",
        "print(f\"Test accuracy: {round(accuracy_SPL * 100, 2)}%\")\n",
        "print(f\"Test top 5 accuracy: {round(top_5_accuracy_SPL * 100, 2)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Llmndpwc2hVI"
      },
      "source": [
        "*   Compute confusion matrix and classification summary of both the models\n",
        "*   Compare both the models with respect to the computed metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR0WHJXpNH9g"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-PGXE2mL8H-"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U56nxPa2ywK"
      },
      "source": [
        "*   Compare DeiT with ViT.\n",
        "*   What are the differences in both the architectures.\n",
        "*   What is one limitation of ViT that is overcome with a DeiT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiblE8UVNSdi"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBd21JmGNU3D"
      },
      "source": [
        "*   What is a SWIN Transformer?\n",
        "*   How is it different from DeiT and ViT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0pApNlJNTQL"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxoKmgAv26_8"
      },
      "source": [
        "*   What is distillation?\n",
        "*   Explain the distillation used in DeiT\n",
        "*   What is its significance? Why do we need distillation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvuDY_lANTqu"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0ZcGNZk3YEB"
      },
      "source": [
        "*   Which is better as a teacher model? A transformer based teacher of a CNN based teacher?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEWQ8qfNNT7T"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnN_t5Me7N5O"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## **End of lab10: Vision Transformers**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}